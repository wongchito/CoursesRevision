{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Technologies for Data Science (INFR11145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of Information Retrieval\n",
    "\n",
    "*Given query $Q$, find relevant documents $D$.*\n",
    "\n",
    "**Main issues:**\n",
    "- Effectiveness\n",
    "    - need to find relevant documents\n",
    "    - different from relational DBs\n",
    "    \n",
    "|&nbsp;|IR|DB|\n",
    "|---|---|---|\n",
    "|**Retrieving**|*Unstructured* data, free text with metadata|*Stuctured* data, clear semantics based on formal model|\n",
    "|**Queries**|Natural language, boolean|Formally-defined (SQL), unambiguous.|\n",
    "|**Result**|Imprecise (need to measure relevance)|Exact|\n",
    "|**Interaction with system**|Important|One-shot queries|\n",
    "\n",
    "- Efficiency\n",
    "    - need to find them quickly\n",
    "    - data constantly changes, need to keep up\n",
    "\n",
    "**Components:**\n",
    "- Documents\n",
    "    - *element* to be retrieved\n",
    "    - unstructured nature\n",
    "    - unique ID\n",
    "    - $N$ documents $\\rightarrow$ Collection\n",
    "- Queries\n",
    "    - text to express *information need*\n",
    "    - same information needs - multiple queries: North Carolina storm vs Florence\n",
    "    - same query - multiple information needs: Apple\n",
    "    - different forms: keywords, sample image, tune\n",
    "- Relevant documents\n",
    "    - similar vocabulary $\\rightarrow$ similar meaning \n",
    "    - similarity: string match, word overlap, retrieval model $P(D|Q)$\n",
    "    - challenges: not clear semantics (Shakespeare), inherent ambiguity (e.g. in queries), highly subjective\n",
    "\n",
    "**Bag-of-words:**\n",
    "- Re-ordering doesn't destroy topic\n",
    "- negations lost\n",
    "- not work for all languages (Chinese, images, music)\n",
    "\n",
    "**Systems perspective:**\n",
    "- Indexing process (offline)\n",
    "- Search (retrieval) process (online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laws of Text\n",
    "\n",
    "### Zipf's Law\n",
    "\n",
    "$$r \\times P_r \\cong \\text{const} \\quad \\Leftrightarrow \\quad P_r \\cong \\frac{\\text{const}}{r}$$\n",
    "\n",
    "- $r$: rank of term according to frequency\n",
    "- $P_r$: probability of appearance of term\n",
    "- Reciprocal proportion\n",
    "- $\\approx$ 50% terms appears once\n",
    "\n",
    "### Benford's Law\n",
    "\n",
    "$$P(d) = \\log (1 + \\frac{1}{d})$$\n",
    "\n",
    "- $d$: first digit of frequency\n",
    "- $P(d)$: pmf of $d$\n",
    "\n",
    "### Heap's Law\n",
    "\n",
    "$$v(n) = k \\times n^b$$\n",
    "\n",
    "- $n$: number of words\n",
    "- $v$: number of unique words in the $n$ words\n",
    "- $b$: typically $\\in (0.4,0.7)$\n",
    "- Accurate for most collections with different $k$ and $b$\n",
    "- Not ccurate when $n$ is small\n",
    "- Not saturated because of spelling errors, names, emails, codes, etc. \n",
    "\n",
    "### Contagion\n",
    "\n",
    "- Most words do not appear that much\n",
    "- Once you see a word $\\rightarrow$ expect to ee again\n",
    "- Like rare contagious disease\n",
    "- Terms appearing twice close to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "*Identify the optimal form of the term to be indexed to achieve the best retrieval performance.*\n",
    "\n",
    "### Tokenisation\n",
    "- Split at non-letter characters\n",
    "- For French, abstract\n",
    "- For German, compound splitter\n",
    "- For Chinese or Japanese, segmentation\n",
    "- Special setup in some applications (e.g. hashtags in social media)\n",
    "\n",
    "\n",
    "### Stopping\n",
    "- Stop words: the most common words (e.g. the, a, is)\n",
    "- $\\approx$ 30%~40% of text\n",
    "- Less influence on topic\n",
    "- New stop words in specific domains (e.g. 'RT' in tweets)\n",
    "- Trend to keep stop words in web search\n",
    "    - good compression techniques\n",
    "    - good query optimisation techniques\n",
    "    - probabilistic retrieval models give low wait\n",
    "\n",
    "### Normalisation\n",
    "- Case folding\n",
    "- Accents removal (e.g. caf√©)\n",
    "- Equivalence classes (e.g. Ph.D. vs PhD)\n",
    "- Be consistent between documents and queries\n",
    "\n",
    "### Stemming\n",
    "- Reduce morphological variations of words\n",
    "    - inflectional (plurals, tenses)\n",
    "    - derivational (verbs nouns)\n",
    "- Basic types\n",
    "    - Dictionary-based: use lists of related words\n",
    "    - Algorithmic\n",
    "        - remove suffixes in English\n",
    "        - but false negatives or false positives exist\n",
    "- Usually achieve 5%~10% retrieval effectiveness improvement in English\n",
    "- Even higher in e.g. Fininsh or Arabic\n",
    "\n",
    "### Limitations\n",
    "- Irregular verbs (e.g. saw vs see)\n",
    "- Different spellings (e.g. colour vs color)\n",
    "- Synonyms (e.g. car vs vehicle)\n",
    "- Solution: query expansion\n",
    "    - more vocabulary, longer query\n",
    "    - potentially more powerful, but less efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "**Document vectors:**\n",
    "- Each vector is a document\n",
    "- Each value is frequency/binary of each term\n",
    "- Collection matrix: all vectors\n",
    "![](document_vectors.png)\n",
    "(Adpated from [Lecture 5: Indexing (1) - Text Technologies for Data Science)](https://www.inf.ed.ac.uk/teaching/courses/tts/handouts2018/05Indexing.pdf)\n",
    "\n",
    "**Term vectors:**\n",
    "- Each vector is a term\n",
    "- Each value is frequency/binary in each document\n",
    "- Transpose of collection matrix\n",
    "![](inverted_vectors.png)\n",
    "(Adpated from [Lecture 5: Indexing (1) - Text Technologies for Data Science)](https://www.inf.ed.ac.uk/teaching/courses/tts/handouts2018/05Indexing.pdf)\n",
    "\n",
    "**Collection matrix:**\n",
    "- Terms against documents\n",
    "- Extremely sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index\n",
    "- Sparse representation of collection matrix\n",
    "- Construction\n",
    "    1. Term sequence\n",
    "    1. Sorting by term then by docID\n",
    "    1. Posting\n",
    "- Inverted index\n",
    "```python\n",
    "{term: list(docID)}\n",
    "```\n",
    "- Inverted index with frequency\n",
    "```python\n",
    "{term: list({docID: count})}\n",
    "```\n",
    "- Proximity index\n",
    "```python\n",
    "{term: list(tuple(docID, term pos))}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching \n",
    "\n",
    "#### Boolean search\n",
    "- Boolean: exist / not-exist\n",
    "- Multiword search: logical operators (AND, OR, NOT)\n",
    "- Query processing: linear merge $O(n)$\n",
    "```python\n",
    "for i in set(index[term1] + index[term2]):\n",
    "    if i in index[term1] and i in index[term2]:\n",
    "        print i\n",
    "```\n",
    "\n",
    "#### Phrase search\n",
    "- Bi-gram index\n",
    "    - index size will explode\n",
    "- Use proximity search (with proximity 1)\n",
    "\n",
    "#### Proximity search\n",
    "- `#3(term1, term2)`\n",
    "- Query processing: linear merge + check terms positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extent index\n",
    "\n",
    "- Special term (e.g. link)\n",
    "```python\n",
    "{link: list(tuple(docID, start, end))}\n",
    "```\n",
    "- Special field (e.g. headline)\n",
    "```python\n",
    "{headline: list(tuple(docID, start, end))}\n",
    "{term: list(tuple(docID, pos+(end-start)))}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index compression\n",
    "\n",
    "- Delta encoding: store ID using delta (difference) from the previous ID\n",
    "- v-byte encoding\n",
    "    - variable byte storage\n",
    "    - high bit is <span style=\"color:red\">1</span>: read following 7 bits\n",
    "    - high bit is <span style=\"color:red\">0</span>: read following 7 bits and check next high bit\n",
    "    - examples\n",
    "        - 6: <span style=\"color:red\">1</span>0000110\n",
    "        - 129: <span style=\"color:red\">0</span>0000001<span style=\"color:red\">1</span>0000001\n",
    "- more compression = less storage = more processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary data structures\n",
    "\n",
    "#### Hashes\n",
    "- Hase each term to a integer\n",
    "- Pro: lookup is faster than tree - $O(1)$\n",
    "- Cons: \n",
    "    - no easy way to find minor variants\n",
    "    - no prefix search\n",
    "    - need to rehasing everything occasionally as vocabulary grows\n",
    "    \n",
    "#### B-tree\n",
    "- Pros:\n",
    "    - solve the prefix problem\n",
    "    - mitigate the rebalacing problem of binary search tree\n",
    "- Cons: slower - $O(\\log M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permuterm indexes\n",
    "\n",
    "- Add \\$ at the end of the term and index all permutations\n",
    "- Add \\$ at the end of wild-card query and permutate until \\* occurs at the end\n",
    "- Look up term\n",
    "\n",
    "Example: `hello`\n",
    "- Indexing: `hello$`, `ello$h`, `llo$he`, `lo$hel`, `o$hell`, `$hello`\n",
    "- Query: `he*o` $\\rightarrow$ `he*o$` $\\rightarrow$ `o$he*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character n-gram indexes\n",
    "\n",
    "Example: `monkey`, `moon` (bigrams)\n",
    "- Indexing: `$m`, `mo`, `on`, `nk`, `ke`, `ey`, `y$`, `$m`, `mo`, `oo`, `on`, `n$`\n",
    "- Query: `mon*`\n",
    "\n",
    "\n",
    "1. Transform query to `$m`, `mo`, `on`\n",
    "1. Find possible terms: `monkey`, `moon`\n",
    "1. Post-filter: eliminate `moon`\n",
    "1. Look up surviving terms `monkey` in document\n",
    "\n",
    "**Application: spelling correction**\n",
    "- E.g. OCR\n",
    "- Possible corrections = most matching results\n",
    "- E.g. elepgant $\\rightarrow$ elegant OR elephant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranked Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean retrieval\n",
    "\n",
    "- Pros: good for expert users with precise understanding of needs (e.g. patent search)\n",
    "- Cons: \n",
    "    - write Boolean queries\n",
    "    - need to go through thousands of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard coeffecient\n",
    "\n",
    "$$\\operatorname{jaccard}(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "- Don't consider term frequency\n",
    "- Treat all terms equally\n",
    "- Driven by length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "\\begin{align}\n",
    "\\operatorname{df}(t) &= \\sum_{i=1}^N \\mathbb 1(t \\in d_i) \\\\\n",
    "\\operatorname{idf}(t) &= \\log_{10} \\frac{N}{\\operatorname{df}(t)} \\\\\n",
    "\\operatorname{tf}(t,d_i) &= \\#\\{t \\in d_i\\} \\\\\n",
    "w_{t,d_i} &= \\left( 1 + \\log_{10} \\operatorname{tf}(t,d_i) \\right) \\times \\operatorname{idf}(t) \\\\\n",
    "\\operatorname{score}(q,d_i) &= \\sum_{t \\in q \\cap d_i} w_{t,d_i}\n",
    "\\end{align}\n",
    "\n",
    "- $\\operatorname{idf}$ (inverse document frequency) measures the importance of a term $t$ in a collection $\\{d_i\\}$\n",
    "- $\\operatorname{tf}$ (term frequency) measures the importance of a term $t$ in a document $d$\n",
    "- $\\operatorname{cf}$ (collection frequency, $\\displaystyle \\sum_{i=1}^N \\# \\{t \\in d_i\\}$) is less commonly used in IR\n",
    "- SMART notation: `ddd.qqq`\n",
    "    - Example: `lnc.ltc`\n",
    "        - For documents: logarithm of tf, no df (1), cosine normalisation\n",
    "        - For queries: logarithm of tf, inverse df, cosine normalisation\n",
    "    - Table of acronyms\n",
    "    \n",
    "|Term Frequency|Document Frequency|Normalisation|\n",
    "|---|---|---|\n",
    "|`n`: natural|`n`: no (1)|`n`: none (1)|\n",
    "|`l`: logarithm|`t`: idf|`c`: cosine|\n",
    "|`a`: augmented|`p`: prob idf (like ReLU)|`u`: pivoted unqiue|\n",
    "|`b`: boolean|&nbsp;|`b`: byte size|\n",
    "|`L`: log ave|&nbsp;|&nbsp;|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector space model (VSM)\n",
    "\n",
    "\\begin{align}\n",
    "M &= \\#\\{q \\cap d_i\\} \\\\\n",
    "\\mathbf q &= (w_{t_1, q}, w_{t_2,q}, \\ldots, w_{t_M, q})^\\top \\\\\n",
    "\\mathbf d_i &= (w_{t_1, d_i}, w_{t_2, d_i}, \\ldots, w_{t_M, d_i})^\\top \\\\\n",
    "\\operatorname{score}(q, d_i) &= \\cos(\\mathbf q, \\mathbf d_i) = \\frac{\\mathbf q^\\top \\mathbf d_i}{\\|\\mathbf q\\|_2 \\|\\mathbf d_i\\|_2} = \\frac{\\sum_{t \\in q \\cap d_i} w_{t,d_i} w_{t,q}}{\\sqrt{\\sum_{j=1}^M w_{t_j,q}^2} \\sqrt{\\sum_{j=1}^M w_{t_j,d_i}^2}}\n",
    "\\end{align}\n",
    "\n",
    "- Heuristic\n",
    "- No notion of relevance\n",
    "- Any weighting scheme, similarity measure can be used\n",
    "- Components are not interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability ranking principle (PRP)\n",
    "\n",
    "- Rank docs by probability of relevance: $P(R|D_{r_1}) > P(R|D_{r_2}) > \\cdots$\n",
    "- Estimate probability as accurate as possible: $\\hat P(R|D) \\approx P(R|D)$\n",
    "- Estimate with all possibly available data: $\\hat P(R|D, Q, \\text{Session}, \\text{Context}, \\text{User profile}, \\ldots)$\n",
    "- Classification problem: $D$ is relevant if $P(R=0|D) > P(R=1|D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okapi BM25 model\n",
    "\n",
    "$$w_{t,d_i} = \\frac{\\operatorname{tf}(t,d_i)}{k \\frac{L_{d_i}}{\\bar L} + \\operatorname{tf}(t,d_i) + 0.5} \\times \\log_{10} \\frac{N - \\operatorname{df}(t) + 0.5}{\\operatorname{df}(t) + 0.5}$$\n",
    "\n",
    "- Best practices $k=1.5$\n",
    "- $\\displaystyle \\bar L = \\frac{1}{N} \\sum_{i=1}^N L_{d_i}$\n",
    "\n",
    "**Assumptions:**\n",
    "- Binary features (term occurrence)\n",
    "- Terms are independence (Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model (LM)\n",
    "\n",
    "- The likelihood $P(q|M_d)$ of generating query $q=\\{t_1, \\ldots, t_K\\}$ from language model $M_d$\n",
    "- Rank docs based on posterior $P(d|q) \\propto P(q|M_d) P(d)$\n",
    "- Prior $P(d)$ often chosen to be Uniform\n",
    "\n",
    "|Model|Likelihood|\n",
    "|---|---|\n",
    "|Unigram LM|$$P(q|M_d) = \\prod_{k=1}^K P(t_k|M_d) = \\prod_{\\text{unique }t} P(t|M_d)^{\\operatorname{tf}(t,q)}$$|\n",
    "|Stochastic LM|$$P(q|M_d) = P(t_1|M_d) \\prod_{k=2}^K P(t_k|M_d, t_{k-1}, \\ldots, t_1)$$|\n",
    "|Bigram LM|$$P(q|M_d) = P(t_1|M_d) \\prod_{k=2}^K P(t_k|M_d, t_{k-1})$$|\n",
    "\n",
    "- MLE $\\displaystyle \\hat P(t|M_d) = \\frac{\\operatorname{tf}(t,d)}{L_d}$ (0 if term not occured in $d$)\n",
    "\n",
    "**Mixture model (Jelinek-mercer smoothing):**\n",
    "\n",
    "$$P(t|d) = \\lambda P(t|M_d) + (1-\\lambda) P(t|M_c)$$\n",
    "\n",
    "- Large $\\lambda$: 'conjunctive-like' search - tends to retrieve docs containing all query words\n",
    "- Small $\\lambda$: more disjunctive, suitable for long queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "|&nbsp;|relevant|irrelevant|\n",
    "|---|---|---|\n",
    "|**retrieved**|TP|FP|\n",
    "|**not retrieved**|FN|TN|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without ranking\n",
    "\n",
    "|Measure|Formula|Notes|\n",
    "|---|---|---|\n",
    "|Precision|$$P=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$|High precision may miss many (less) relevant docs|\n",
    "|Recall|$$R=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$|High recall may contain many irrelevant docs|\n",
    "|Accuracy|$$A=\\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{FP}+\\text{TN}+\\text{FN}}$$|TN dominates the result if #irrelevant >> #relevant|\n",
    "|Balanced F-measure|$$F_1=\\frac{2 \\cdot P \\cdot R}{P+R}$$|Harmonic mean of precision and recall. Emphasises the importance of small values|\n",
    "|General F-measure|$$F_\\beta=\\frac{(\\beta^2+1) \\cdot P \\cdot R}{\\beta^2P + R}$$|Controls the importance of recall (than precision)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ranking (binary relevance)\n",
    "\n",
    "- Precision @ K (P@K): Cut-off ranked list at rank K then calculate precision\n",
    "    - (average badly)\n",
    "- R-Precision: P@R where R is the number of known relevant docs\n",
    "    - R is different given different query\n",
    "    - (not realistic)\n",
    "- Average Precision (AP): $\\displaystyle \\operatorname{AP}(q)=\\frac{1}{\\#\\mathrm{Rel}_q} \\sum_{k=1}^{n} P(k) \\mathbb 1(d_k \\in \\mathrm{Rel}_q)$\n",
    "    - Expectation of P@K where K is uniformly distributed\n",
    "- Mean Average Precision (MAP): $\\displaystyle \\operatorname{MAP} = \\frac{1}{Q} \\sum_{q=1}^Q \\operatorname{AP}(q)$\n",
    "    - A mix between precision and recall\n",
    "    - Highly focus on finding relevant docs as early as possible\n",
    "    - Most commonly used for most IR tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded relevance\n",
    "\n",
    "|Measure|Formula|\n",
    "|---|---|\n",
    "|Gain|$G(k)=$ grade of retrieved doc at rank $k$|\n",
    "|Discounted Gain|$$\\operatorname{DG}(k) = \\begin{cases}\n",
    "\\operatorname{G}(k) & k=1 \\\\\n",
    "\\displaystyle \\frac{\\operatorname{G}(k)}{\\log_2 k} & k \\geq 2\n",
    "\\end{cases}$$|\n",
    "|Discounted Cumulative Gain|$$\\operatorname{DCG}(k) = \\sum_{i=1}^k \\operatorname{DG}(k)$$|\n",
    "|Ideal Gain|$$\\operatorname{iG}(k) = G_{(k)}$$ (should include all relevant docs)|\n",
    "|Ideal DG|$$\\operatorname{iDG}(k) = \\begin{cases}\n",
    "\\operatorname{iG}(k) & k=1 \\\\\n",
    "\\displaystyle \\frac{\\operatorname{iG}(k)}{\\log_2 k} & k \\geq 2\n",
    "\\end{cases}$$|\n",
    "|Ideal DCG|$$\\operatorname{iDCG}(k) = \\sum_{i=1}^k \\operatorname{iDG}(k)$$|\n",
    "|Normalised DCG|$$\\operatorname{nDCG}(k) = \\frac{\\operatorname{DCG}(k)}{\\operatorname{iDCG}(k)}$$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Retrieval Conferences (TREC)\n",
    "\n",
    "- Main IR evaluation compaign\n",
    "- To search a set of docs of given genre and domain\n",
    "- Formed of a set of tracks, each track has a set of search tasks\n",
    "\n",
    "#### Collection\n",
    "\n",
    "- Cover most of the domains in life\n",
    "- A set of hundreds of thousands of docs\n",
    "- Format:\n",
    "```xml\n",
    "<DOC>\n",
    "    <DOCNO>1234</DOCNO>\n",
    "    <TEXT>Plain text of the document</TEXT>\n",
    "</DOC>\n",
    "```\n",
    "\n",
    "#### Topic\n",
    "\n",
    "- Query sets provided for each collection\n",
    "- Generated by experts\n",
    "- Format:\n",
    "```xml\n",
    "<num>189</num>\n",
    "<title>the query text</title>\n",
    "<desc>description of what is meant by the query</desc>\n",
    "<narr>what should be considered relevant</narr>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "1. Each system submit top 1000 docs per topic\n",
    "1. For each system, judge top 100 docs by\n",
    "    - single poll, remove duplicates, random rank\n",
    "    - judged by the person developing the topic\n",
    "1. Treat unevaluated docs as irrelevant\n",
    "1. Compute MAP down to 1000 docs\n",
    "\n",
    "\n",
    "- Large number of reasonable systems\n",
    "- Systems must not all do the same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's kappa\n",
    "\n",
    "$$\\varkappa = \\frac{P(A)-P(E)}{1-P(E)}$$\n",
    "\n",
    "- $P(A) = P(J_1=1, J_2=1) + P(J_1=0, J_2=0)$ (probability of judges agree)\n",
    "- $P(E) = P(J_1=1)P(J_2=1) + P(J_1=0)P(J_2=0)$ (agreement by chance)\n",
    "\n",
    "$$\\varkappa \\in \\begin{cases}\n",
    "\\{1\\} & \\text{total agreement} \\\\\n",
    "(0.8, 1) & \\text{good agreement} \\\\\n",
    "(0.67, 0.8] & \\text{'fair' agreement} \\\\\n",
    "(0, 0.67) & \\text{data providing a suspicious basis for an evaluation} \\\\\n",
    "\\{0\\} & \\text{chance agreement} \\\\\n",
    "[-1,0) & \\text{worse than random}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B testing\n",
    "\n",
    "1. Have most users use old system\n",
    "1. Divert a small proportion of traffic to the new system\n",
    "1. Evaluate with automatic measures\n",
    "1. Use significance test\n",
    "\n",
    "**Paired t-test (one-sided):**\n",
    "\n",
    "$$t = \\frac{\\bar d \\cdot \\sqrt{n}}{\\operatorname{std}(d)} \\sim t_{n-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic thesaurus\n",
    "\n",
    "**Co-occurence:**\n",
    "\n",
    "$$\\mathbf C = \\mathbf{AA}^\\top$$\n",
    "\n",
    "- Pros: unsupervised\n",
    "- Cons: \n",
    "    - related words more than real synonyms\n",
    "    - computationally expensive\n",
    "    \n",
    "**Parallel corpus:**\n",
    "\n",
    "- Sets of two parallel corpus in two different languages (source and target lang)\n",
    "- Main training resource for machine translation systems\n",
    "- Improvement not consistent due to lack of context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance feedback\n",
    "\n",
    "- Users give feedback to IR system (relevant/irrelevant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** s1680642\n",
    "\n",
    "**Licensing:** <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
