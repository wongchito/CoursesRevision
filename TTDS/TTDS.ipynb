{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Technologies for Data Science (INFR11145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "*Given query $Q$, find relevant documents $D$.*\n",
    "\n",
    "**Main issues:**\n",
    "- Effectiveness\n",
    "    - need to find relevant documents\n",
    "    - different from relational DBs\n",
    "    \n",
    "|&nbsp;|IR|DB|\n",
    "|---|---|---|\n",
    "|**Retrieving**|*Unstructured* data, free text with metadata|*Stuctured* data, clear semantics based on formal model|\n",
    "|**Queries**|Natural language, boolean|Formally-defined (SQL), unambiguous.|\n",
    "|**Result**|Imprecise (need to measure relevance)|Exact|\n",
    "|**Interaction with system**|Important|One-shot queries|\n",
    "\n",
    "- Efficiency\n",
    "    - need to find them quickly\n",
    "    - data constantly changes, need to keep up\n",
    "\n",
    "**Components:**\n",
    "- Documents\n",
    "    - *element* to be retrieved\n",
    "    - unstructured nature\n",
    "    - unique ID\n",
    "    - $N$ documents $\\rightarrow$ Collection\n",
    "- Queries\n",
    "    - text to express *information need*\n",
    "    - same information needs - multiple queries: North Carolina storm vs Florence\n",
    "    - same query - multiple information needs: Apple\n",
    "    - different forms: keywords, sample image, tune\n",
    "- Relevant documents\n",
    "    - similar vocabulary $\\rightarrow$ similar meaning \n",
    "    - similarity: string match, word overlap, retrieval model $P(D|Q)$\n",
    "    - challenges: not clear semantics (Shakespeare), inherent ambiguity (e.g. in queries), highly subjective\n",
    "\n",
    "**Bag-of-words:**\n",
    "- Re-ordering doesn't destroy topic\n",
    "- negations lost\n",
    "- not work for all languages (Chinese, images, music)\n",
    "\n",
    "**Systems perspective:**\n",
    "- Indexing process (offline)\n",
    "- Search (retrieval) process (online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laws of text\n",
    "\n",
    "#### Zipf's Law\n",
    "\n",
    "$$r \\times P_r \\cong \\text{const} \\quad \\Leftrightarrow \\quad P_r \\cong \\frac{\\text{const}}{r}$$\n",
    "\n",
    "- $r$: rank of term according to frequency\n",
    "- $P_r$: probability of appearance of term\n",
    "- Reciprocal proportion\n",
    "- $\\approx$ 50% terms appears once\n",
    "\n",
    "#### Benford's Law\n",
    "\n",
    "$$P(d) = \\log (1 + \\frac{1}{d})$$\n",
    "\n",
    "- $d$: first digit of frequency\n",
    "- $P(d)$: pmf of $d$\n",
    "\n",
    "#### Heap's Law\n",
    "\n",
    "$$v(n) = k \\times n^b$$\n",
    "\n",
    "- $n$: number of words\n",
    "- $v$: number of unique words in the $n$ words\n",
    "- $b$: typically $\\in (0.4,0.7)$\n",
    "- Accurate for most collections with different $k$ and $b$\n",
    "- Not ccurate when $n$ is small\n",
    "- Not saturated because of spelling errors, names, emails, codes, etc. \n",
    "\n",
    "#### Contagion\n",
    "\n",
    "- Most words do not appear that much\n",
    "- Once you see a word $\\rightarrow$ expect to ee again\n",
    "- Like rare contagious disease\n",
    "- Terms appearing twice close to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "*Identify the optimal form of the term to be indexed to achieve the best retrieval performance.*\n",
    "\n",
    "#### Tokenisation\n",
    "- Split at non-letter characters\n",
    "- For French, abstract\n",
    "- For German, compound splitter\n",
    "- For Chinese or Japanese, segmentation\n",
    "- Special setup in some applications (e.g. hashtags in social media)\n",
    "\n",
    "\n",
    "#### Stopping\n",
    "- Stop words: the most common words (e.g. the, a, is)\n",
    "- $\\approx$ 30%~40% of text\n",
    "- Less influence on topic\n",
    "- New stop words in specific domains (e.g. 'RT' in tweets)\n",
    "- Trend to keep stop words in web search\n",
    "    - good compression techniques\n",
    "    - good query optimisation techniques\n",
    "    - probabilistic retrieval models give low wait\n",
    "\n",
    "#### Normalisation\n",
    "- Case folding\n",
    "- Accents removal (e.g. caf√©)\n",
    "- Equivalence classes (e.g. Ph.D. vs PhD)\n",
    "- Be consistent between documents and queries\n",
    "\n",
    "#### Stemming\n",
    "- Reduce morphological variations of words\n",
    "    - inflectional (plurals, tenses)\n",
    "    - derivational (verbs nouns)\n",
    "- Basic types\n",
    "    - Dictionary-based: use lists of related words\n",
    "    - Algorithmic\n",
    "        - remove suffixes in English\n",
    "        - but false negatives or false positives exist\n",
    "- Usually achieve 5%~10% retrieval effectiveness improvement in English\n",
    "- Even higher in e.g. Fininsh or Arabic\n",
    "\n",
    "#### Limitations\n",
    "- Irregular verbs (e.g. saw vs see)\n",
    "- Different spellings (e.g. colour vs color)\n",
    "- Synonyms (e.g. car vs vehicle)\n",
    "- Solution: query expansion\n",
    "    - more vocabulary, longer query\n",
    "    - potentially more powerful, but less efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "**Document vectors:**\n",
    "- Each vector is a document\n",
    "- Each value is frequency/binary of each term\n",
    "- Collection matrix: all vectors\n",
    "![](document_vectors.png)\n",
    "\n",
    "**Term vectors:**\n",
    "- Each vector is a term\n",
    "- Each value is frequency/binary in each document\n",
    "- Transpose of collection matrix\n",
    "![](inverted_vectors.png)\n",
    "\n",
    "**Collection matrix:**\n",
    "- Terms against documents\n",
    "- Extremely sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverted index\n",
    "- Sparse representation of collection matrix\n",
    "- Construction\n",
    "    1. Term sequence\n",
    "    1. Sorting by term then by docID\n",
    "    1. Posting\n",
    "- Inverted index\n",
    "```python\n",
    "{term: list(docID)}\n",
    "```\n",
    "- Inverted index with frequency\n",
    "```python\n",
    "{term: list({docID: count})}\n",
    "```\n",
    "- Proximity index\n",
    "```python\n",
    "{term: list(tuple(docID, term pos))}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching \n",
    "\n",
    "##### Boolean search\n",
    "- Boolean: exist / not-exist\n",
    "- Multiword search: logical operators (AND, OR, NOT)\n",
    "- Query processing: linear merge $O(n)$\n",
    "```python\n",
    "for i in set(index[term1] + index[term2]):\n",
    "    if i in index[term1] and i in index[term2]:\n",
    "        print i\n",
    "```\n",
    "\n",
    "##### Phrase search\n",
    "- Bi-gram index\n",
    "    - index size will explode\n",
    "- Use proximity search (with proximity 1)\n",
    "\n",
    "##### Proximity search\n",
    "- `#3(term1, term2)`\n",
    "- Query processing: linear merge + check terms positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extent index\n",
    "\n",
    "- Special term (e.g. link)\n",
    "```python\n",
    "{link: list(tuple(docID, start, end))}\n",
    "```\n",
    "- Special field (e.g. headline)\n",
    "```python\n",
    "{headline: list(tuple(docID, start, end))}\n",
    "{term: list(tuple(docID, pos+(end-start)))}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index compression\n",
    "\n",
    "- Delta encoding: store ID using delta (difference) from the previous ID\n",
    "- v-byte encoding\n",
    "    - variable byte storage\n",
    "    - high bit is <span style=\"color:red\">1</span>: read following 7 bits\n",
    "    - high bit is <span style=\"color:red\">0</span>: read following 7 bits and check next high bit\n",
    "    - examples\n",
    "        - 6: <span style=\"color:red\">1</span>0000110\n",
    "        - 129: <span style=\"color:red\">0</span>0000001<span style=\"color:red\">1</span>0000001\n",
    "- more compression = less storage = more processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary data structures\n",
    "\n",
    "##### Hashes\n",
    "- Hase each term to a integer\n",
    "- Pro: lookup is faster than tree - $O(1)$\n",
    "- Cons: \n",
    "    - no easy way to find minor variants\n",
    "    - no prefix search\n",
    "    - need to rehasing everything occasionally as vocabulary grows\n",
    "    \n",
    "##### B-tree\n",
    "- Pros:\n",
    "    - solve the prefix problem\n",
    "    - mitigate the rebalacing problem of binary search tree\n",
    "- Cons: slower - $O(\\log M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permuterm indexes\n",
    "\n",
    "- Add \\$ at the end of the term and index all permutations\n",
    "- Add \\$ at the end of wild-card query and permutate until \\* occurs at the end\n",
    "- Look up term\n",
    "\n",
    "Example: `hello`\n",
    "- Indexing: `hello$`, `ello$h`, `llo$he`, `lo$hel`, `o$hell`, `$hello`\n",
    "- Query: `he*o` $\\rightarrow$ `he*o$` $\\rightarrow$ `o$he*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character n-gram indexes\n",
    "\n",
    "Example: `monkey`, `moon` (bigrams)\n",
    "- Indexing: `$m`, `mo`, `on`, `nk`, `ke`, `ey`, `y$`, `$m`, `mo`, `oo`, `on`, `n$`\n",
    "- Query: `mon*`\n",
    "\n",
    "\n",
    "1. Transform query to `$m`, `mo`, `on`\n",
    "1. Find possible terms: `monkey`, `moon`\n",
    "1. Post-filter: eliminate `moon`\n",
    "1. Look up surviving terms `monkey` in document\n",
    "\n",
    "**Application: spelling correction**\n",
    "- E.g. OCR\n",
    "- Possible corrections = most matching results\n",
    "- E.g. elepgant $\\rightarrow$ elegant OR elephant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranked retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean retrieval\n",
    "\n",
    "- Pros: good for expert users with precise understanding of needs (e.g. patent search)\n",
    "- Cons: \n",
    "    - write Boolean queries\n",
    "    - need to go through thousands of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard coeffecient\n",
    "\n",
    "$$\\operatorname{jaccard}(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "- Don't consider term frequency\n",
    "- Treat all terms equally\n",
    "- Driven by length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF\n",
    "\n",
    "\\begin{align}\n",
    "\\operatorname{df}(t) &= \\sum_{i=1}^N \\mathbb 1(t \\in d_i) \\\\\n",
    "\\operatorname{idf}(t) &= \\log_{10} \\frac{N}{\\operatorname{df}(t)} \\\\\n",
    "\\operatorname{tf}(t,d_i) &= \\#\\{t \\in d_i\\} \\\\\n",
    "w_{t,d_i} &= \\left( 1 + \\log_{10} \\operatorname{tf}(t,d_i) \\right) \\times \\operatorname{idf}(t) \\\\\n",
    "\\operatorname{score}(q,d_i) &= \\sum_{t \\in q \\cap d_i} w_{t,d_i}\n",
    "\\end{align}\n",
    "\n",
    "- $\\operatorname{idf}$ (inverse document frequency) measures the importance of a term $t$ in a collection $\\{d_i\\}$\n",
    "- $\\operatorname{tf}$ (term frequency) measures the importance of a term $t$ in a document $d$\n",
    "- $\\operatorname{cf}$ (collection frequency, $\\displaystyle \\sum_{i=1}^N \\# \\{t \\in d_i\\}$) is less commonly used in IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector space model (VSM)\n",
    "\n",
    "\\begin{align}\n",
    "M &= \\#\\{q \\cap d_i\\} \\\\\n",
    "\\mathbf q &= (w_{t_1, q}, w_{t_2,q}, \\ldots, w_{t_M, q})^\\top \\\\\n",
    "\\mathbf d_i &= (w_{t_1, d_i}, w_{t_2, d_i}, \\ldots, w_{t_M, d_i})^\\top \\\\\n",
    "\\operatorname{score}(q, d_i) &= \\cos(\\mathbf q, \\mathbf d_i) = \\frac{\\mathbf q^\\top \\mathbf d_i}{\\|\\mathbf q\\|_2 \\|\\mathbf d_i\\|_2} = \\frac{\\sum_{t \\in q \\cap d_i} w_{t,d_i} w_{t,q}}{\\sqrt{\\sum_{j=1}^M w_{t_j,q}^2} \\sqrt{\\sum_{j=1}^M w_{t_j,d_i}^2}}\n",
    "\\end{align}\n",
    "\n",
    "- Heuristic\n",
    "- No notion of relevance\n",
    "- Any weighting scheme, similarity measure can be used\n",
    "- Components are not interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability ranking principle (PRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** s1680642\n",
    "\n",
    "**Licensing:** <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
