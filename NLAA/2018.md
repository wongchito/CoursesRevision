# Systems of Linear Equations (LSE)
## __Basic Algorithm__
* __DP (Dot product)__
    * input: $\mathbf x, \mathbf y \in \mathbb R^{n}$, output: $s= \mathbf x^T \mathbf y$
    * $C(n)=2n=O(n)$ 

* __MV (Matrix-vector multiplication)__
    * input: $\mathbf A \in \mathbb R^{m \times n}, \mathbf x \in \mathbb R^n$, output: $\mathbf{y=Ax}$
    * $C(m,n)=m \times 2n=O(mn)$ 
* __MM (Matrix-matrix multiplication)__
    * input: $\mathbf A \in \mathbb R^{m \times n}, \mathbf B \in \mathbb R^{n \times p}$, output: $\mathbf{C=AB}$
    * $C(m,n,p)=m \times p \times 2n=O(mnp)$
* __Forward Substitution__
    * 用于 lower triangular
    * $C(n)=n^2$
* __Backward Substitution__
    * 用于 upper triangular
    * $C(n)=n^2$ 
## __Norm__
* __Vector norm__
    * $\displaystyle ||\mathbf x||_2= \sqrt {\sum_{i=1}^n x_i^2}= \sqrt{x^Tx} \quad$ (Euclidean norm)
    * $\displaystyle ||\mathbf x||_p= \left (\sum_{i=1}^n |x_i|^p \right)^{1/p}$
    * $\displaystyle ||\mathbf x||_\infty= \underset{1 \leq i \leq n}{\max} |x_i|= \lim_{p \to \infty} ||\mathbf x||_p$

* __Matrix norm__
    * induced norm: $\displaystyle \| \mathbf A\|= \underset{\mathbf{x \neq 0}}{max} \frac{\|\mathbf{Ax}\|}{\|\mathbf x\|}$
    * $\displaystyle ||\mathbf A||_\infty = \underset{1 \leq i \leq n}{max} \sum_{j=1}^n |a_{ij}|$
    * $\displaystyle ||\mathbf A||_1= \underset{1 \leq j \leq n}{max} \sum _{i=1}^n |a_{ij}|$
    * $\displaystyle ||\mathbf A||_2= \sqrt {\rho(\mathbf A^T \mathbf A)}$, $\rho(\mathbf A^T \mathbf A)$是$\mathbf A^T \mathbf A$的max eigenvalue

## __Gaussian Elimination & LU factorisation__
* __LU factorisation__
    * $\mathbf L$: unit, lower triangular 
      
      $\mathbf U$: non-singular, upper triangular
    * $C(n)=\frac{2}{3}n^3+ \frac{1}{2}n^2- \frac{7}{6}n$
* __GE__
    * find $\mathbf{A=LU}$ 
      
      solve $\mathbf{Ly=b}$ using forward substitution 
      
      solve $\mathbf{Ux=y}$ using backward substitution
    * $C(n)=\frac{2}{3}n^3+ \frac{5}{2}n^2- \frac{7}{6}n=O(n^3)$
    * Error analysis
        * $\hat x$ is the computed solution, $(\mathbf A+\Delta \mathbf A) \hat{\mathbf x}=\mathbf b$
        * not backward-stable

 

## __Gaussian Elimination with Partial Pivoting__
* __PA=LU factorisation__
    * $\mathbf P$: permutation matrix, every row and every column contains $n-1$ zeros and $1$ one, orthogonal
      
      $\mathbf L$: unit, lower triangular 
      
      $\mathbf U$: non-singular, upper triangular

*  __GEPP__
    * find $\mathbf{PA+LU}$
      
      solve $\mathbf{Ly=Pb}$ using forward substitution

      solve $\mathbf{Ux=y}$ using backward substitution 
    * $C(n)=O(n^3)$, additional cost for choosing the pivot $i$.
      
      Partial pivoting: find the maximum of $(n-k+1)$ numbers, add $O(n^2)$ comparisons, negligible.
      
      Complete pivoting: find the maximum of $(n-k+1)^2$ numbers, add $O(n^3)$ comparisons, not negligible.
    * Error analysis
        * $(\mathbf A+\Delta \mathbf A) \hat{\mathbf x}=\mathbf b$, for $\varepsilon_m$ sufficiently small,
       
          $||\Delta \mathbf A||_\infty \leq 2^{n+1} \varepsilon_m ||\mathbf A||_\infty$
        * backward-stable
## __Condition numbers__
* __Square matrix $\mathbf A \in \mathbb R^{n \times n}$__
    * Definition: $\kappa(\mathbf A)=$ 
    $\begin{cases} 
     \|\mathbf A\| \ \|\mathbf A^{-1}\| & \text{if } \mathbf A \text{ is invertible} \\
      \infty & \text{otherwise}
      \end{cases}$  
    * ill-conditioned: if $\kappa (\mathbf A)$ is large. 
    * Relative error: $\displaystyle \frac{||\mathbf{x-\hat x}||}{||\mathbf x||} \leq \frac{\kappa (\mathbf A)}{1-\kappa (\mathbf A)\frac{||\Delta \mathbf A||}{||\mathbf A||}} \cdot \frac{||\Delta \mathbf A||}{||\mathbf A||}$
    * $\mathbf A$ with eigenvalues $|\lambda_1| \geq |\lambda_2| \geq \ldots \geq |\lambda_n|$ and corresponding eigenvectors $\mathbf x_1, \ldots , \mathbf x_n$. If $\mathbf x_1, \ldots , \mathbf x_n$ form an orthonormal set, then
   
      $\displaystyle \kappa_2 (\mathbf A)= \|\mathbf A\|_2 \|\mathbf A^{-1}\|_2=\frac{|\lambda_1|}{|\lambda_n|} \geq 1$

* __Rectangular matrix $\mathbf A \in \mathbb R^{m \times n} (m \geq n)$__
   * Definition: $\kappa_p(\mathbf A)=$ 
    $\begin{cases} 
     \|\mathbf A\|_p \ \|\mathbf A^\dagger\|_p & \text{if rank} (\mathbf A)=n \\
      \infty & \text{otherwise}
      \end{cases}$  
   * $\displaystyle \kappa_2 (\mathbf A)= \|\mathbf A\|_2 \|\mathbf A^\dagger \|_2=\frac{\sigma_1}{\sigma_n} \geq 1$





## __QR factorisation__
* __QR factorisation__
    * $\mathbf Q$: orthogonal
    
    * $\mathbf R$: non-singular, upper triangular
* __GS (Gram-Schmidt orthonormalisation)__
    * $C(n)=2n^3+n^2+n$  
    * Error analysis: 
        * $\mathbf{\hat Q}^T \mathbf{\hat Q}=\mathbf I+\Delta \mathbf I, \mathbf{\hat Q \hat R}= \mathbf A+\Delta \mathbf A$ 
        * not backward stable 

* __MGS (Modified Gram-Schmidt orthonormalisation)__
    * $C(n)=2n^3+n^2+n$
    * Error analysis: 
        * $\mathbf{\hat Q}^T \mathbf{\hat Q}=\mathbf I+\Delta \mathbf I, \mathbf{\hat Q \hat R}= \mathbf A+\Delta \mathbf A$, for $\varepsilon_m$ sufficiently small, 
        
            $||\Delta \mathbf I||_2 \leq \alpha_1(n) \varepsilon_m \kappa_2 (\mathbf A)= \alpha_1(n) \varepsilon_m \|\mathbf A\|_2 \|\mathbf A^{-1}\|_2$
        
            $||\Delta \mathbf A||_2 \leq  \alpha_2(n) \varepsilon_m \|\mathbf A\|_2$
        *  backward stable 
* __SQR__
    * find $\mathbf {A=QR}$ 
    
      calculate $\mathbf{y=Q}^T \mathbf b$ 
      
      solve $\mathbf{Rx=y}$ using backward substitution
    * $C(n)=2n^3+5n^2+n$ 


## __Iterative Methods__
* __General iterative method__
    *  write $\mathbf {A=M+N}$, where $\mathbf M$ is easier to invert than $\mathbf A$.
    
    *  Error: $\mathbf e_k= \mathbf{x-x}_k$. Residual: $\mathbf r_k = \mathbf b- \mathbf{Ax}_k$
    
    * 收敛条件: $\mathbf e_k= \mathbf R^k \mathbf e_0$, if $\|\mathbf R\|_p < 1$, then $\|\mathbf e_k\|_p \to 0$ as $k \to \infty$.
* __Jacobi method__
    *  write $\mathbf {A=L+D+U}$, choosing $\mathbf {M=D}$ and $\mathbf{N=L+U}$.
       
       $\mathbf L$: strictly lower triangular
       
       $\mathbf D$: diagonal
       
       $\mathbf U$: strictly upper triangular
    * each iteration: $2n^2+2n=O(n^2)$
   
      $C(n)=O(k_{max} n^2)$, $k_{max}$ is the number of iterations, which depends on the chosen accuracy $\varepsilon_r$ and on the properties of the matrix $\mathbf A$, $k_{max}$ will be large if $\mathbf A$ is ill-conditioned.
    * Error analysis: $\mathbf A$ 严格对角占优 $\to$ converges
* __Gauss-Seidel method__
    *  write $\mathbf {A=L+D+U}$, choosing $\mathbf {M=L+D}$ and $\mathbf{N=U}$.
       
       $\mathbf L$: strictly lower triangular
       
       $\mathbf D$: diagonal
       
       $\mathbf U$: strictly upper triangular 
    * each iteration: $3n^2+n=O(n^2)$ 
     
      Larger than Jacobi, but GS is usually still faster since it requires fewer iterations.
    * Error analysis: $\mathbf A$ 严格对角占优 $\to$ converges



# Least Squares Problems (LSQ)
## __Least Squares Problem__
* Find $\mathbf x \in \mathbb R^n$ that minimises $\|\mathbf{Ax-b}\|_2$.

* __Normal equations__
    * $\mathbf A^T \mathbf{Ax} = \mathbf A^T \mathbf b$
    * If $\mathbf x \in \mathbb R^n$ minimises $\|\mathbf{Ax-b}\|_2$, then it solves the normal equations.
    * Assume: $m \geq n$ and rank$(\mathbf A)=n \to \mathbf A^T \mathbf A$ is invertible $\to$ the normal equations have a unique solution $\mathbf x=(\mathbf A^T \mathbf A)^{-1} \mathbf A^T \mathbf b$
    * Moore-Penrose pseudo-inverse: $\mathbf A^\dagger=(\mathbf A^T \mathbf A)^{-1} \mathbf A^T \in \mathbb R^{n \times m}$
       
      If $\mathbf A$ is invertible, then $\mathbf A^{-1}= \mathbf A ^\dagger$.

## __Singular Value Decomposition__
* __SVD__
    * $\mathbf A= \mathbf{U \Sigma V}^T$
     
      $\mathbf U$: orthogonal, its columns are the __left singular values__ of $\mathbf A$.
      
      $\mathbf \Sigma$: diagonal, with diagonal entries $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_p \geq 0$, where $p=$min{$m,n$}, $\sigma_1, \sigma_2, \ldots$ are the __singular values__ of $\mathbf A$.
      
      $\mathbf V$: orthogonal, its columns are the __right singular values__ of $\mathbf A$.
    * $\displaystyle \kappa_2 (\mathbf A)= \|\mathbf A\|_2 \|\mathbf A^\dagger \|_2=\frac{\sigma_1}{\sigma_n} \geq 1$
    * $\kappa_2 (\mathbf A^T \mathbf A)=\kappa_2 (\mathbf A)^2$
* __LSQ via QR__
    * Reduced QR factorisation
      
      $\underset{m \times n}{\mathbf A} = \underset{m \times m}{\mathbf Q} \ \underset{m \times n}{\mathbf R}= [\mathbf Q_1 \ \mathbf Q_2] \begin{bmatrix} \mathbf R_1 \\ \mathbf 0 \end{bmatrix}=\underset{m \times n}{\mathbf Q_1} \ \underset{n \times n}{\mathbf R_1}$ 
    * compute reduced QR factorisation $\mathbf A= \mathbf Q_1 \mathbf R_1$ 

      compute $\mathbf y= \mathbf Q_1^T \mathbf b \in \mathbb R^n$ 

      solve $\mathbf R_1 \mathbf {x=y}$ using backward substitution
    * $C(n)=O(nm^2)$
* __LSQ via SVD__ 
   * Reduced SVD
      
      $\underset{m \times n}{\mathbf A} = \underset{m \times m}{\mathbf U} \ \underset{m \times n}{\mathbf \Sigma} \ \underset{n \times n}{\mathbf V}^T= [\mathbf U_1 \ \mathbf U_2] \begin{bmatrix} \mathbf \Sigma_1 \\ \mathbf 0 \end{bmatrix} \mathbf V^T=\underset{m \times n}{\mathbf U_1} \ \underset{n \times n}{\mathbf \Sigma_1} \ \mathbf V^T$ 
    * compute reduced SVD $\mathbf A= \mathbf Q_1 \mathbf R_1$ 

      compute $\mathbf z= \mathbf U_1^T \mathbf b \in \mathbb R^n$ 

      solve $\mathbf \Sigma_1 \mathbf {y=z}$  using backward substitution

      compute $\mathbf{x=Vy}$
   


# Eigenvalue Problems (EVP) 
## __Eigenvalue Problem (symmetric matrices)__
* find $\lambda \in \mathbb C$ and $\mathbf x \in \mathbb C^n \backslash \{0\}$ s.t. $\mathbf{Ax}=\lambda \mathbf x$.
   
  Characteristic polynomial: $\xi_{\mathbf A}(\lambda)=det(\mathbf A- \lambda \mathbf I)$
* If $\mathbf A$ is symmetric, then the eigenvalues are real + the eigenvectors can be chosen to form an orthonormal basis of $\mathbb R^n$
* Rayleigh quotient: $\displaystyle r_{\mathbf A}(\mathbf x)= \frac{\mathbf x^T \mathbf{Ax}}{\mathbf x^T \mathbf x}$. We have $r_{\mathbf A}(\mathbf x_i)=\lambda_i$.
* 求eigenvector: 解方程 $(\mathbf A- \lambda_i \mathbf I) \mathbf x_i=\mathbf 0$
* 求eigenvalue: 求 $r_{\mathbf A}(\mathbf x_i)$




## __Iteration__
* __PI (Power iteration)__
    * Calculate an approximate eigenvector as $\displaystyle \mathbf z^{(k)}= \frac{\mathbf A^k \mathbf z^{(0)}}{\| \mathbf A^k \mathbf z^{(0)}\|_2}$, $\mathbf z^{(k)}$ is normalised in every step of the iteration.
    * initial guess: $\| \mathbf z^{(0)}\|_2=1$
    * $\mathbf w^{(k)}=\mathbf{Az}^{(k-1)}$
       
      $\displaystyle \mathbf z^{(k)}= \frac{\mathbf w^{(k)}}{\|\mathbf w^{(k)}\|_2}$

      $\lambda^{(k)}=r_{\mathbf A}(\mathbf z^{(k)})$
    * Speed of convergence: depend on $\displaystyle \frac{|\lambda_2|}{|\lambda_1|}$
    * each iteration: $C(n)=O(n^2)$
* __SII (Shifted inverse iteration)__
    * Compute the eigenvalue of $\mathbf A$ closest to $s$ (the eigenvalues of $(\mathbf A-s \mathbf I)^{-1}$ are $\mu_i= (\lambda_i-s)^{-1}, i=1,\ldots,n$)
      
    * set $s=0 \to$ __Inverse iteration__ (Power iteration applied to $\mathbf A^{-1}$)
    * initial guess: $\|\mathbf z^{(0)}\|_2=1$
    * $\mathbf w^{(k)}=(\mathbf A-s \mathbf I)^{-1}z^{(k-1)}$
       
      $\displaystyle \mathbf z^{(k)}= \frac{\mathbf w^{(k)}}{\|\mathbf w^{(k)}\|_2}$

      $\lambda^{(k)}=r_{\mathbf A}(\mathbf z^{(k)})$
    * Speed of convergence: depend on $\displaystyle \frac{|(\lambda_a-s)^{-1}|}{|(\lambda_b-s)^{-1}|}$
    * each iteration: $C(n)=O(n^3)$, more expensive than PI
* __SIMI (Simultaneous iteration)__
    * Compute all eigenvectors at once, is to choose $\mathbf Z ^{(0)} \in \mathbb R^{n \times n}$ orthogonal, and to multiply $\mathbf Z^{(0)}$ repeatedly by $\mathbf A$.
    * initial guess: orthogonal matrix $\mathbf Z ^{(0)} \in \mathbb R^{n \times n}$
    * $\mathbf W^{(k)}=\mathbf{AZ}^{(k-1)}$
       
      calculate QR factorisation $\mathbf W^{(k)}=\mathbf Z^{(k)} \mathbf R^{(k)}$

      $\mathbf \Lambda^{(k)}=(\mathbf Z^{(k)})^T\mathbf{AZ}^{(k)}$
    * Speed of convergence: depend on $\displaystyle \underset{1 \leq l \leq n-1}{max} \frac{|\lambda_{l+1}|}{|\lambda_l|}$
    * each iteration: $C(n)=O(n^3)$