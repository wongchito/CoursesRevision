{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Linear Algebra (Revision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems of Linear Equations (LSE)\n",
    "\n",
    "## Basic Algorithm\n",
    "* __DP (Dot product)__\n",
    "    * input: $\\mathbf x, \\mathbf y \\in \\mathbb R^{n}$, output: $s= \\mathbf x^T \\mathbf y$\n",
    "    * $C(n)=2n=O(n)$ \n",
    "\n",
    "* __MV (Matrix-vector multiplication)__\n",
    "    * input: $\\mathbf A \\in \\mathbb R^{m \\times n}, \\mathbf x \\in \\mathbb R^n$, output: $\\mathbf{y=Ax}$\n",
    "    * $C(m,n)=m \\times 2n=O(mn)$ \n",
    "* __MM (Matrix-matrix multiplication)__\n",
    "    * input: $\\mathbf A \\in \\mathbb R^{m \\times n}, \\mathbf B \\in \\mathbb R^{n \\times p}$, output: $\\mathbf{C=AB}$\n",
    "    * $C(m,n,p)=m \\times p \\times 2n=O(mnp)$\n",
    "* __Forward Substitution__\n",
    "    * 用于 lower triangular\n",
    "    * $C(n)=n^2$\n",
    "* __Backward Substitution__\n",
    "    * 用于 upper triangular\n",
    "    * $C(n)=n^2$ \n",
    "    \n",
    "## Norm\n",
    "* __Vector norm__\n",
    "    * $\\displaystyle ||\\mathbf x||_2= \\sqrt {\\sum_{i=1}^n x_i^2}= \\sqrt{x^Tx} \\quad$ (Euclidean norm)\n",
    "    * $\\displaystyle ||\\mathbf x||_p= \\left (\\sum_{i=1}^n |x_i|^p \\right)^{1/p}$\n",
    "    * $\\displaystyle ||\\mathbf x||_\\infty= \\underset{1 \\leq i \\leq n}{\\max} |x_i|= \\lim_{p \\to \\infty} ||\\mathbf x||_p$\n",
    "\n",
    "* __Matrix norm__\n",
    "    * induced norm: $\\displaystyle \\| \\mathbf A\\|= \\underset{\\mathbf{x \\neq 0}}{max} \\frac{\\|\\mathbf{Ax}\\|}{\\|\\mathbf x\\|}$\n",
    "    * $\\displaystyle ||\\mathbf A||_\\infty = \\underset{1 \\leq i \\leq n}{max} \\sum_{j=1}^n |a_{ij}|$\n",
    "    * $\\displaystyle ||\\mathbf A||_1= \\underset{1 \\leq j \\leq n}{max} \\sum _{i=1}^n |a_{ij}|$\n",
    "    * $\\displaystyle ||\\mathbf A||_2= \\sqrt {\\rho(\\mathbf A^T \\mathbf A)}$, $\\rho(\\mathbf A^T \\mathbf A)$是$\\mathbf A^T \\mathbf A$的max eigenvalue\n",
    "\n",
    "## Gaussian Elimination & LU factorisation\n",
    "* __LU factorisation__\n",
    "    * $\\mathbf L$: unit, lower triangular \n",
    "      \n",
    "    - $\\mathbf U$: non-singular, upper triangular\n",
    "    * $C(n)=\\frac{2}{3}n^3+ \\frac{1}{2}n^2- \\frac{7}{6}n$\n",
    "* __GE__\n",
    "    * find $\\mathbf{A=LU}$ \n",
    "    - solve $\\mathbf{Ly=b}$ using forward substitution \n",
    "    - solve $\\mathbf{Ux=y}$ using backward substitution\n",
    "    * $C(n)=\\frac{2}{3}n^3+ \\frac{5}{2}n^2- \\frac{7}{6}n=O(n^3)$\n",
    "    * Error analysis\n",
    "        * $\\hat x$ is the computed solution, $(\\mathbf A+\\Delta \\mathbf A) \\hat{\\mathbf x}=\\mathbf b$\n",
    "        * not backward-stable\n",
    "\n",
    " \n",
    "## Gaussian Elimination with Partial Pivoting\n",
    "* __PA=LU factorisation__\n",
    "    * $\\mathbf P$: permutation matrix, every row and every column contains $n-1$ zeros and $1$ one, orthogonal\n",
    "    - $\\mathbf L$: unit, lower triangular \n",
    "    - $\\mathbf U$: non-singular, upper triangular\n",
    "\n",
    "*  __GEPP__\n",
    "    * find $\\mathbf{PA+LU}$\n",
    "    - solve $\\mathbf{Ly=Pb}$ using forward substitution\n",
    "    - solve $\\mathbf{Ux=y}$ using backward substitution \n",
    "    * $C(n)=O(n^3)$, additional cost for choosing the pivot $i$.\n",
    "    - Partial pivoting: find the maximum of $(n-k+1)$ numbers, add $O(n^2)$ comparisons, negligible.\n",
    "    - Complete pivoting: find the maximum of $(n-k+1)^2$ numbers, add $O(n^3)$ comparisons, not negligible.\n",
    "    * Error analysis\n",
    "        * $(\\mathbf A+\\Delta \\mathbf A) \\hat{\\mathbf x}=\\mathbf b$, for $\\varepsilon_m$ sufficiently small,\n",
    "       \n",
    "          $||\\Delta \\mathbf A||_\\infty \\leq 2^{n+1} \\varepsilon_m ||\\mathbf A||_\\infty$\n",
    "        * backward-stable\n",
    "        \n",
    "## Condition numbers\n",
    "* __Square matrix $\\mathbf A \\in \\mathbb R^{n \\times n}$__\n",
    "    * Definition: $\\kappa(\\mathbf A)=$ \n",
    "    $\\begin{cases} \n",
    "     \\|\\mathbf A\\| \\ \\|\\mathbf A^{-1}\\| & \\text{if } \\mathbf A \\text{ is invertible} \\\\\n",
    "      \\infty & \\text{otherwise}\n",
    "      \\end{cases}$  \n",
    "    * ill-conditioned: if $\\kappa (\\mathbf A)$ is large. \n",
    "    * Relative error: $\\displaystyle \\frac{||\\mathbf{x-\\hat x}||}{||\\mathbf x||} \\leq \\frac{\\kappa (\\mathbf A)}{1-\\kappa (\\mathbf A)\\frac{||\\Delta \\mathbf A||}{||\\mathbf A||}} \\cdot \\frac{||\\Delta \\mathbf A||}{||\\mathbf A||}$\n",
    "    * $\\mathbf A$ with eigenvalues $|\\lambda_1| \\geq |\\lambda_2| \\geq \\ldots \\geq |\\lambda_n|$ and corresponding eigenvectors $\\mathbf x_1, \\ldots , \\mathbf x_n$. If $\\mathbf x_1, \\ldots , \\mathbf x_n$ form an orthonormal set, then\n",
    "   \n",
    "      $\\displaystyle \\kappa_2 (\\mathbf A)= \\|\\mathbf A\\|_2 \\|\\mathbf A^{-1}\\|_2=\\frac{|\\lambda_1|}{|\\lambda_n|} \\geq 1$\n",
    "\n",
    "* __Rectangular matrix $\\mathbf A \\in \\mathbb R^{m \\times n} (m \\geq n)$__\n",
    "   * Definition: $\\kappa_p(\\mathbf A)=$ \n",
    "    $\\begin{cases} \n",
    "     \\|\\mathbf A\\|_p \\ \\|\\mathbf A^\\dagger\\|_p & \\text{if rank} (\\mathbf A)=n \\\\\n",
    "      \\infty & \\text{otherwise}\n",
    "      \\end{cases}$  \n",
    "   * $\\displaystyle \\kappa_2 (\\mathbf A)= \\|\\mathbf A\\|_2 \\|\\mathbf A^\\dagger \\|_2=\\frac{\\sigma_1}{\\sigma_n} \\geq 1$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## QR factorisation\n",
    "* __QR factorisation__\n",
    "    * $\\mathbf Q$: orthogonal\n",
    "    \n",
    "    * $\\mathbf R$: non-singular, upper triangular\n",
    "* __GS (Gram-Schmidt orthonormalisation)__\n",
    "    * $C(n)=2n^3+n^2+n$  \n",
    "    * Error analysis: \n",
    "        * $\\mathbf{\\hat Q}^T \\mathbf{\\hat Q}=\\mathbf I+\\Delta \\mathbf I, \\mathbf{\\hat Q \\hat R}= \\mathbf A+\\Delta \\mathbf A$ \n",
    "        * not backward stable \n",
    "\n",
    "* __MGS (Modified Gram-Schmidt orthonormalisation)__\n",
    "    * $C(n)=2n^3+n^2+n$\n",
    "    * Error analysis: \n",
    "        * $\\mathbf{\\hat Q}^T \\mathbf{\\hat Q}=\\mathbf I+\\Delta \\mathbf I, \\mathbf{\\hat Q \\hat R}= \\mathbf A+\\Delta \\mathbf A$, for $\\varepsilon_m$ sufficiently small, \n",
    "        \n",
    "            $||\\Delta \\mathbf I||_2 \\leq \\alpha_1(n) \\varepsilon_m \\kappa_2 (\\mathbf A)= \\alpha_1(n) \\varepsilon_m \\|\\mathbf A\\|_2 \\|\\mathbf A^{-1}\\|_2$\n",
    "        \n",
    "            $||\\Delta \\mathbf A||_2 \\leq  \\alpha_2(n) \\varepsilon_m \\|\\mathbf A\\|_2$\n",
    "        *  backward stable \n",
    "* __SQR__\n",
    "    * find $\\mathbf {A=QR}$ \n",
    "    \n",
    "      calculate $\\mathbf{y=Q}^T \\mathbf b$ \n",
    "      \n",
    "      solve $\\mathbf{Rx=y}$ using backward substitution\n",
    "    * $C(n)=2n^3+5n^2+n$ \n",
    "\n",
    "\n",
    "## Iterative Methods\n",
    "* __General iterative method__\n",
    "    *  write $\\mathbf {A=M+N}$, where $\\mathbf M$ is easier to invert than $\\mathbf A$.\n",
    "    \n",
    "    *  Error: $\\mathbf e_k= \\mathbf{x-x}_k$. Residual: $\\mathbf r_k = \\mathbf b- \\mathbf{Ax}_k$\n",
    "    \n",
    "    * 收敛条件: $\\mathbf e_k= \\mathbf R^k \\mathbf e_0$, if $\\|\\mathbf R\\|_p < 1$, then $\\|\\mathbf e_k\\|_p \\to 0$ as $k \\to \\infty$.\n",
    "* __Jacobi method__\n",
    "    *  write $\\mathbf {A=L+D+U}$, choosing $\\mathbf {M=D}$ and $\\mathbf{N=L+U}$.\n",
    "       \n",
    "       $\\mathbf L$: strictly lower triangular\n",
    "       \n",
    "       $\\mathbf D$: diagonal\n",
    "       \n",
    "       $\\mathbf U$: strictly upper triangular\n",
    "    * each iteration: $2n^2+2n=O(n^2)$\n",
    "   \n",
    "      $C(n)=O(k_{max} n^2)$, $k_{max}$ is the number of iterations, which depends on the chosen accuracy $\\varepsilon_r$ and on the properties of the matrix $\\mathbf A$, $k_{max}$ will be large if $\\mathbf A$ is ill-conditioned.\n",
    "    * Error analysis: $\\mathbf A$ 严格对角占优 $\\to$ \bconverges\n",
    "* __Gauss-Seidel method__\n",
    "    *  write $\\mathbf {A=L+D+U}$, choosing $\\mathbf {M=L+D}$ and $\\mathbf{N=U}$.\n",
    "       \n",
    "       $\\mathbf L$: strictly lower triangular\n",
    "       \n",
    "       $\\mathbf D$: diagonal\n",
    "       \n",
    "       $\\mathbf U$: strictly upper triangular \n",
    "    * each iteration: $3n^2+n=O(n^2)$ \n",
    "     \n",
    "      Larger than Jacobi, but GS is usually still faster since it requires fewer iterations.\n",
    "    * Error analysis: $\\mathbf A$ 严格对角占优 $\\to$ converges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Problems (LSQ)\n",
    "## Least Squares Problem\n",
    "* Find $\\mathbf x \\in \\mathbb R^n$ that minimises $\\|\\mathbf{Ax-b}\\|_2$.\n",
    "\n",
    "* __Normal equations__\n",
    "    * $\\mathbf A^T \\mathbf{Ax} = \\mathbf A^T \\mathbf b$\n",
    "    * If $\\mathbf x \\in \\mathbb R^n$ minimises $\\|\\mathbf{Ax-b}\\|_2$, then it solves the normal equations.\n",
    "    * Assume: $m \\geq n$ and rank$(\\mathbf A)=n \\to \\mathbf A^T \\mathbf A$ is invertible $\\to$ the normal equations have a unique solution $\\mathbf x=(\\mathbf A^T \\mathbf A)^{-1} \\mathbf A^T \\mathbf b$\n",
    "    * Moore-Penrose pseudo-inverse: $\\mathbf A^\\dagger=(\\mathbf A^T \\mathbf A)^{-1} \\mathbf A^T \\in \\mathbb R^{n \\times m}$\n",
    "       \n",
    "      If $\\mathbf A$ is invertible, then $\\mathbf A^{-1}= \\mathbf A ^\\dagger$.\n",
    "\n",
    "## Singular Value Decomposition\n",
    "* __SVD__\n",
    "    * $\\mathbf A= \\mathbf{U \\Sigma V}^T$\n",
    "     \n",
    "      $\\mathbf U$: orthogonal, its columns are the __left singular values__ of $\\mathbf A$.\n",
    "      \n",
    "      $\\mathbf \\Sigma$: diagonal, with diagonal entries $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_p \\geq 0$, where $p=$min{$m,n$}, $\\sigma_1, \\sigma_2, \\ldots$ are the __singular values__ of $\\mathbf A$.\n",
    "      \n",
    "      $\\mathbf V$: orthogonal, its columns are the __right singular values__ of $\\mathbf A$.\n",
    "    * $\\displaystyle \\kappa_2 (\\mathbf A)= \\|\\mathbf A\\|_2 \\|\\mathbf A^\\dagger \\|_2=\\frac{\\sigma_1}{\\sigma_n} \\geq 1$\n",
    "    * $\\kappa_2 (\\mathbf A^T \\mathbf A)=\\kappa_2 (\\mathbf A)^2$\n",
    "* __LSQ via QR__\n",
    "    * Reduced QR factorisation\n",
    "      \n",
    "      $\\underset{m \\times n}{\\mathbf A} = \\underset{m \\times m}{\\mathbf Q} \\ \\underset{m \\times n}{\\mathbf R}= [\\mathbf Q_1 \\ \\mathbf Q_2] \\begin{bmatrix} \\mathbf R_1 \\\\ \\mathbf 0 \\end{bmatrix}=\\underset{m \\times n}{\\mathbf Q_1} \\ \\underset{n \\times n}{\\mathbf R_1}$ \n",
    "    * compute reduced QR factorisation $\\mathbf A= \\mathbf Q_1 \\mathbf R_1$ \n",
    "\n",
    "      compute $\\mathbf y= \\mathbf Q_1^T \\mathbf b \\in \\mathbb R^n$ \n",
    "\n",
    "      solve $\\mathbf R_1 \\mathbf {x=y}$ using backward substitution\n",
    "    * $C(n)=O(nm^2)$\n",
    "* __LSQ via SVD__ \n",
    "   * Reduced SVD\n",
    "      \n",
    "      $\\underset{m \\times n}{\\mathbf A} = \\underset{m \\times m}{\\mathbf U} \\ \\underset{m \\times n}{\\mathbf \\Sigma} \\ \\underset{n \\times n}{\\mathbf V}^T= [\\mathbf U_1 \\ \\mathbf U_2] \\begin{bmatrix} \\mathbf \\Sigma_1 \\\\ \\mathbf 0 \\end{bmatrix} \\mathbf V^T=\\underset{m \\times n}{\\mathbf U_1} \\ \\underset{n \\times n}{\\mathbf \\Sigma_1} \\ \\mathbf V^T$ \n",
    "    * compute reduced SVD $\\mathbf A= \\mathbf Q_1 \\mathbf R_1$ \n",
    "\n",
    "      compute $\\mathbf z= \\mathbf U_1^T \\mathbf b \\in \\mathbb R^n$ \n",
    "\n",
    "      solve $\\mathbf \\Sigma_1 \\mathbf {y=z}$  using backward substitution\n",
    "\n",
    "      compute $\\mathbf{x=Vy}$\n",
    "   \n",
    "\n",
    "\n",
    "# Eigenvalue Problems (EVP) \n",
    "## Eigenvalue Problem (symmetric matrices)\n",
    "* find $\\lambda \\in \\mathbb C$ and $\\mathbf x \\in \\mathbb C^n \\backslash \\{0\\}$ s.t. $\\mathbf{Ax}=\\lambda \\mathbf x$.\n",
    "   \n",
    "  Characteristic polynomial: $\\xi_{\\mathbf A}(\\lambda)=det(\\mathbf A- \\lambda \\mathbf I)$\n",
    "* If $\\mathbf A$ is symmetric, then the eigenvalues are real + the eigenvectors can be chosen to form an orthonormal basis of $\\mathbb R^n$\n",
    "* Rayleigh quotient: $\\displaystyle r_{\\mathbf A}(\\mathbf x)= \\frac{\\mathbf x^T \\mathbf{Ax}}{\\mathbf x^T \\mathbf x}$. We have $r_{\\mathbf A}(\\mathbf x_i)=\\lambda_i$.\n",
    "* 求eigenvector: 解方程 $(\\mathbf A- \\lambda_i \\mathbf I) \\mathbf x_i=\\mathbf 0$\n",
    "* 求eigenvalue: 求 $r_{\\mathbf A}(\\mathbf x_i)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iteration\n",
    "* __PI (Power iteration)__\n",
    "    * Calculate an approximate eigenvector as $\\displaystyle \\mathbf z^{(k)}= \\frac{\\mathbf A^k \\mathbf z^{(0)}}{\\| \\mathbf A^k \\mathbf z^{(0)}\\|_2}$, $\\mathbf z^{(k)}$ is normalised in every step of the iteration.\n",
    "    * initial guess: $\\| \\mathbf z^{(0)}\\|_2=1$\n",
    "    * $\\mathbf w^{(k)}=\\mathbf{Az}^{(k-1)}$\n",
    "       \n",
    "      $\\displaystyle \\mathbf z^{(k)}= \\frac{\\mathbf w^{(k)}}{\\|\\mathbf w^{(k)}\\|_2}$\n",
    "\n",
    "      $\\lambda^{(k)}=r_{\\mathbf A}(\\mathbf z^{(k)})$\n",
    "    * \bSpeed of convergence: depend on $\\displaystyle \\frac{|\\lambda_2|}{|\\lambda_1|}$\n",
    "    * each iteration: $C(n)=O(n^2)$\n",
    "* __SII (Shifted inverse iteration)__\n",
    "    * Compute the eigenvalue of $\\mathbf A$ closest to $s$ (the eigenvalues of $(\\mathbf A-s \\mathbf I)^{-1}$ are $\\mu_i= (\\lambda_i-s)^{-1}, i=1,\\ldots,n$)\n",
    "      \n",
    "    * set $s=0 \\to$ __Inverse iteration__ (Power iteration applied to $\\mathbf A^{-1}$)\n",
    "    * initial guess: $\\|\\mathbf z^{(0)}\\|_2=1$\n",
    "    * $\\mathbf w^{(k)}=(\\mathbf A-s \\mathbf I)^{-1}z^{(k-1)}$\n",
    "       \n",
    "      $\\displaystyle \\mathbf z^{(k)}= \\frac{\\mathbf w^{(k)}}{\\|\\mathbf w^{(k)}\\|_2}$\n",
    "\n",
    "      $\\lambda^{(k)}=r_{\\mathbf A}(\\mathbf z^{(k)})$\n",
    "    * Speed of convergence: depend on $\\displaystyle \\frac{|(\\lambda_a-s)^{-1}|}{|(\\lambda_b-s)^{-1}|}$\n",
    "    * each iteration: $C(n)=O(n^3)$, more expensive than PI\n",
    "* __SIMI (Simultaneous iteration)__\n",
    "    * Compute all eigenvectors at once, is to choose $\\mathbf Z ^{(0)} \\in \\mathbb R^{n \\times n}$ orthogonal, and to multiply $\\mathbf Z^{(0)}$ repeatedly by $\\mathbf A$.\n",
    "    * initial guess: orthogonal matrix $\\mathbf Z ^{(0)} \\in \\mathbb R^{n \\times n}$\n",
    "    * $\\mathbf W^{(k)}=\\mathbf{AZ}^{(k-1)}$\n",
    "       \n",
    "      calculate QR factorisation $\\mathbf W^{(k)}=\\mathbf Z^{(k)} \\mathbf R^{(k)}$\n",
    "\n",
    "      $\\mathbf \\Lambda^{(k)}=(\\mathbf Z^{(k)})^T\\mathbf{AZ}^{(k)}$\n",
    "    * Speed of convergence: depend on $\\displaystyle \\underset{1 \\leq l \\leq n-1}{max} \\frac{|\\lambda_{l+1}|}{|\\lambda_l|}$\n",
    "    * each iteration: $C(n)=O(n^3)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** s1889985\n",
    "\n",
    "**Licensing:** <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
