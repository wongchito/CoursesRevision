{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining and Exploration (INFR 11007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps in EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1: 0, 1, 1, 1, 2, 3, 4, 4, 5, 9\n",
      "Set 2: 0, 1, 1, 1, 2, 3, 4, 4, 5, 9000\n"
     ]
    }
   ],
   "source": [
    "set1 = np.array([0, 1, 1, 1, 2, 3, 4, 4, 5, 9])\n",
    "set2 = np.array([0, 1, 1, 1, 2, 3, 4, 4, 5, 9000])\n",
    "print('Set 1: {}'.format(', '.join(list(map(str, set1)))))\n",
    "print('Set 2: {}'.format(', '.join(list(map(str, set2)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location\n",
    "\n",
    "**Not-robust measures:**\n",
    "- Sample mean: $\\displaystyle \\bar x = \\frac{1}{n} \\sum_{i=1}^n x_i$\n",
    "    - Estimator of the mean of r.v. $X$\n",
    "    \n",
    "**Robust measures:**\n",
    "- Median: $\\displaystyle \\mathrm{median}(x) = \\begin{cases}\n",
    "    x_{(\\frac{n+1}{2})} & \\text{if } n \\text{ is odd} \\\\\n",
    "    \\frac{1}{2} \\left( x_{(\\frac{n}{2})} + x_{(\\frac{n}{2}+1)} \\right) & \\text{if } n \\text{ is even}\n",
    "\\end{cases}$\n",
    "- Mode: the value that occurs most frequently\n",
    "- Quantile: $q_\\alpha \\approx x_{(\\lceil n\\alpha \\rceil)}$\n",
    "    - Quartile: $Q_1 = q_{0.25}$, $Q_2 = q_{0.5}$, $Q_3 = q_{0.75}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>$Q_1$</th>\n",
       "      <th>$Q_2$</th>\n",
       "      <th>$Q_3$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Set 1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Set 2</th>\n",
       "      <td>902.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean  median  $Q_1$  $Q_2$  $Q_3$\n",
       "Set 1    3.0     2.5    1.0    2.5    4.0\n",
       "Set 2  902.1     2.5    1.0    2.5    4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'mean': [set1.mean(), set2.mean()], \n",
    "    'median': [np.median(set1), np.median(set2)], \n",
    "    '$Q_1$': [np.quantile(set1, 0.25), np.quantile(set2, 0.25)], \n",
    "    '$Q_2$': [np.quantile(set1, 0.5), np.quantile(set2, 0.5)], \n",
    "    '$Q_3$': [np.quantile(set1, 0.75), np.quantile(set2, 0.75)]\n",
    "}\n",
    "pd.DataFrame(data=d, index=['Set 1','Set 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale\n",
    "\n",
    "**Not-robust measures:**\n",
    "- Sample variance: $\\displaystyle \\mathrm{Var}(x) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar x)^2$\n",
    "    - Estimator of the variance of r.v. $X$\n",
    "- Sample standard deviation: $\\mathrm{std}(x) = \\sqrt{\\mathrm{Var}(x)}$\n",
    "\n",
    "**Robust measures:**\n",
    "- Median absolute deviation: $\\mathrm{MAD}(x) = \\mathrm{median}(|x_i - \\mathrm{median}(x)|)$\n",
    "- Interquartile range: $\\mathrm{IQR} = Q_3 - Q_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def mad(x):\n",
    "    return np.median(np.abs(x - np.median(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>std</th>\n",
       "      <th>MAD</th>\n",
       "      <th>IQR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Set 1</th>\n",
       "      <td>6.40</td>\n",
       "      <td>2.529822</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Set 2</th>\n",
       "      <td>7286222.89</td>\n",
       "      <td>2699.300445</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variance          std  MAD  IQR\n",
       "Set 1        6.40     2.529822  1.5  3.0\n",
       "Set 2  7286222.89  2699.300445  1.5  3.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'variance': [set1.var(), set2.var()], \n",
    "    'std': [set1.std(), set2.std()], \n",
    "    'MAD': [mad(set1), mad(set2)], \n",
    "    'IQR': [iqr(set1), iqr(set2)]\n",
    "}\n",
    "pd.DataFrame(data=d, index=['Set 1','Set 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape\n",
    "\n",
    "**Not-robust measures:**\n",
    "- Sample skewness: $\\displaystyle \\mathrm{skew}(x) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i - \\bar x}{\\mathrm{std}(x)} \\right)^3$\n",
    "    - Location and scale are not taken into account\n",
    "- Sample kurtosis: $\\displaystyle \\mathrm{kurt}(x) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i - \\bar x}{\\mathrm{std}(x)} \\right)^4$\n",
    "\n",
    "**Robust measures:**\n",
    "- Galtonâ€™s measure of skewness: $\\displaystyle \\frac{(Q_3 - Q_2) - (Q_2 - Q_1)}{Q_3 - Q_1}$\n",
    "- Robust kurtosis: $\\displaystyle \\frac{(q_{7/8} - q_{5/8}) + (q_{3/8} - q_{1/8})}{Q_3 - Q_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def Galton(x):\n",
    "    q1 = np.quantile(x, 0.25)\n",
    "    q2 = np.quantile(x, 0.5)\n",
    "    q3 = np.quantile(x, 0.75)\n",
    "    return ((q3-q2)-(q2-q1))/(q3-q1)\n",
    "\n",
    "def robust_kurt(x):\n",
    "    q1 = np.quantile(x, 1/8)\n",
    "    q2 = np.quantile(x, 1/4)\n",
    "    q3 = np.quantile(x, 3/8)\n",
    "    q5 = np.quantile(x, 5/8)\n",
    "    q6 = np.quantile(x, 3/4)\n",
    "    q7 = np.quantile(x, 7/8)\n",
    "    return ((q7-q5)+(q3-q1))/(q6-q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skewness</th>\n",
       "      <th>Galton</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>robustKurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Set 1</th>\n",
       "      <td>1.074680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525391</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Set 2</th>\n",
       "      <td>2.666665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.111106</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       skewness  Galton  kurtosis  robustKurt\n",
       "Set 1  1.074680     0.0  0.525391    0.541667\n",
       "Set 2  2.666665     0.0  5.111106    0.541667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'skewness': [skew(set1), skew(set2)], \n",
    "    'Galton': [Galton(set1), Galton(set2)], \n",
    "    'kurtosis': [kurtosis(set1), kurtosis(set2)],\n",
    "    'robustKurt': [robust_kurt(set1), robust_kurt(set2)]\n",
    "}\n",
    "pd.DataFrame(data=d, index=['Set 1','Set 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate measures\n",
    "\n",
    "**Not-robust measures:**\n",
    "- Sample covariance: $\\displaystyle \\mathrm{cov}(x,y) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y)$\n",
    "    - Estimator of the covariance of r.v.s $X$ and $Y$\n",
    "- Pearson's correlation coefficient: $\\displaystyle \\rho(x, y) = \\frac{\\mathrm{cov}(x,y)}{\\mathrm{std}(x) \\mathrm{std}(y)}$\n",
    "    - $-1 \\leq \\rho \\leq 1$\n",
    "    - Aka linear correlation coefficient because linear relation is measured. \n",
    "    - Nonlinear: $\\displaystyle \\rho(g(x), g(y)) = \\frac{\\mathrm{cov}(g(x),g(y))}{\\mathrm{std}(g(x)) \\mathrm{std}(g(y))}$\n",
    "- Sample covariance matrix\n",
    "    - Positive semi-definite\n",
    "    - Total variance $\\displaystyle \\sum_{i=1}^d \\mathrm{Var}(x_i) = \\sum_{i=1}^d \\lambda_i$ (by eigenvalue decomopsition $\\mathrm{Cov}(\\mathbf x) = \\mathbf U \\boldsymbol\\Lambda \\mathbf U^\\top$)\n",
    "- Sample correlation matrix: $\\displaystyle \\rho(\\mathbf x) = \\mathrm{diag} \\left( \\frac{1}{\\mathrm{std}(\\mathbf x)} \\right) \\mathrm{Cov}(\\mathbf x) \\mathrm{diag} \\left( \\frac{1}{\\mathrm{std}(\\mathbf x)} \\right)$\n",
    "\n",
    "**Robust measures:**\n",
    "- Kendall's $\\tau$: $\\displaystyle \\tau(x,y) = \\frac{n_c(x,y) - n_d(x,y)}{n(n-1)/2}$\n",
    "    - $n_c(x,y)$ is the number of concordant pairs, i.e. $\\#\\left\\{(x_i, y_i) \\text{ and } (x_j, y_j), \\forall i \\neq j\\right\\}$ s.t. $\\displaystyle \\begin{cases}\n",
    "x_i > x_j \\\\ \n",
    "y_i > y_j\n",
    "\\end{cases} \\text{ or } \\begin{cases}\n",
    "x_i < x_j \\\\\n",
    "y_i < y_j\n",
    "\\end{cases}$\n",
    "    - $n_d(x,y)$ is the number of discordant pairs, i.e. $\\#\\left\\{(x_i, y_i) \\text{ and } (x_j, y_j), \\forall i \\neq j\\right\\}$ s.t. $\\displaystyle \\begin{cases}\n",
    "x_i > x_j \\\\ \n",
    "y_i < y_j\n",
    "\\end{cases} \\text{ or } \\begin{cases}\n",
    "x_i < x_j \\\\\n",
    "y_i > y_j\n",
    "\\end{cases}$\n",
    "    - If $\\begin{cases}\n",
    "x_i = x_j \\\\\n",
    "y_i = y_j\n",
    "\\end{cases}$, the pair is neither concordant nor discordant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot\n",
    "\n",
    "- Number of occurences of an attribute\n",
    "- More useful to show relevant frequencies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box plot\n",
    "\n",
    "- Based on robust measures (quartiles)\n",
    "\n",
    "<embed src=\"boxplot.pdf\" type=\"application/pdf\" width=\"500px\" height=\"520px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "\n",
    "$$B_i = [L+(i-1)h, L+ih) \\quad i = 1, \\ldots, k$$\n",
    "\n",
    "- To visualise whold dataset, $L \\leq \\min (x_1, \\ldots, x_n)$ and $L+kh \\geq \\max (x_1, \\ldots, x_n)$\n",
    "- Different starting values lead to differently looking histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel density plot\n",
    "\n",
    "$$\\hat p(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i)$$\n",
    "\n",
    "- Kernel: $\\displaystyle \\int K_h(x) \\mathrm dx = 1$\n",
    "- Bandwidth $h$: $\\displaystyle K_h(x) = \\frac{1}{h}K(\\frac{1}{h})$\n",
    "- Gaussian kernel: $\\displaystyle K_h (\\xi) = \\frac{1}{\\sqrt{2 \\pi h^2}} \\exp \\left( -\\frac{\\xi^2}{2h^2} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin plot\n",
    "\n",
    "- Combination of [box plot](#Box-plot) (robust) and [kernel density plot](#Kernel-density-plot) (non-robust but informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "ax = sns.violinplot(x=\"species\", y=\"sepal_length\", data=iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation\n",
    "\n",
    "**Centring matrix**\n",
    "\n",
    "$$\\tilde {\\mathbf X} = \\mathbf{X C}_n$$\n",
    "- $\\mathbf C_n = \\mathbf I_n - \\frac{1}{n} \\boldsymbol 1_n \\boldsymbol 1_n^\\top$\n",
    "- $\\mathbf C_n \\mathbf C_n = \\mathbf C_n$\n",
    "- Multiply from the left instead removes the sample mean of each column\n",
    "\n",
    "**Scaling to unit variance**\n",
    "\n",
    "$$\\mathbf z_i = \\mathrm{diag} \\left( \\frac{1}{\\mathrm{std}(\\mathbf x)} \\right) \\tilde{\\mathbf x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier\n",
    "\n",
    "- Turkey's fences $$[Q_1 - k(Q_3-Q_1), Q_3 + k(Q_3-Q_1)] = [Q_1 - k \\mathrm{IQR}(x), Q_3 + k \\mathrm{IQR}(x)]$$\n",
    "    - $k \\geq 0$ (most commonly $k=1.5$)\n",
    "    - Used in [box plots](#Box-plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance maximisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequtial\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{First PC: } & \\begin{cases}\n",
    "\\underset{\\mathbf w_1}{\\text{maximise}} & \\mathrm{Var}(z_1) = \\mathrm{Var}(\\mathbf w_1^\\top \\mathbf x) = \\mathbf w_1^\\top \\boldsymbol\\Sigma \\mathbf w_1 \\\\\n",
    "\\text{subject to} & \\| \\mathbf w_1 \\| = 1\n",
    "\\end{cases} \\\\\n",
    "\\text{Subsequent PCs: } & \\begin{cases}\n",
    "\\underset{\\mathbf w_m}{\\text{maximise}} & \\mathbf w_m^\\top \\boldsymbol\\Sigma \\mathbf w_m \\\\\n",
    "\\text{subject to} & \\| \\mathbf w_m \\| = 1 \\\\\n",
    "& \\mathbf w_m^\\top \\mathbf w_1 = 0 \\quad i = 1, \\ldots, m-1\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "1. Let $\\mathbf w_1 = \\mathbf{Ua}$, then $\\displaystyle \\mathbf w_1^\\top \\boldsymbol\\Sigma \\mathbf w_1 = \\sum_{i=1}^d a_i^2 \\lambda_i$ and $\\displaystyle \\| \\mathbf w_1 \\| = \\sum_{i=1}^d a_i^2 = 1$\n",
    "1. $\\mathbf a = (1, 0, \\ldots, 0)^\\top$ is the solution of the optimisation problem if $\\lambda_1 > \\lambda_i$\n",
    "1. $\\mathbf w_1 = \\mathbf u_1 = \\mathbf{Ue}_1$ is the first PC direction, with variance $\\lambda_1$\n",
    "1. For subsequent calculations, constraints $a_i=0 \\ (i=1,\\ldots,m-1)$ exist\n",
    "\n",
    "\n",
    "- PCs (scores) uncorrelated: \n",
    "\\begin{align*}\n",
    "\\mathbb E [z_i z_j] &= \\mathbb E [\\mathbf w_i^\\top \\mathbf{xx}^\\top \\mathbf w_j] \\\\\n",
    "&= \\mathbf w_i^\\top \\boldsymbol\\Sigma \\mathbf w_j \\\\\n",
    "&= \\mathbf e_i^\\top \\mathbf U^\\top \\mathbf U \\boldsymbol\\Lambda \\mathbf U^\\top \\mathbf{Ue}_j \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "- Fraction of variance explained: $\\displaystyle \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^d \\lambda_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simultaneous\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\mathbf w_1, \\ldots, \\mathbf w_k}{\\text{maximise}} &\\quad \\sum_{i=1}^k \\mathbf w_i^\\top \\boldsymbol\\Sigma \\mathbf w_i \\\\\n",
    "\\text{subject to} &\\quad \\| \\mathbf w_i \\| = 1 &\\quad i = 1, \\ldots, k \\\\\n",
    "&\\quad \\mathbf w_i^\\top \\mathbf w_j = 0 &\\quad i \\neq j\n",
    "\\end{align*}\n",
    "\n",
    "- Subtle techinical point: not using greedy algorithms as in [sequential approach](#Sequential), which are not guaranteed to yeild optimal solutions\n",
    "- However, same results as [sequential approach](#Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation error minimisation\n",
    "\n",
    "Approximation error: $\\displaystyle \\mathbb E \\| \\mathbf x - \\mathbf{Px} \\|^2 = \\mathbb E \\| \\mathbf x - \\mathbf W_k \\mathbf W_k^\\top \\mathbf x \\|^2 = \\mathbb E \\| \\mathbf x - \\sum_{i=1}^k \\mathbf w_k \\mathbf w_k^\\top \\mathbf x \\|^2$\n",
    "- $\\underset{d\\times k}{\\mathbf W_k} = (\\mathbf w_1, \\ldots, \\mathbf w_k)$, where $\\mathbf w_i$ are orthogonal vectors spanning a $k$-dim subspace of $\\mathbb R^d$\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\mathbf w_1, \\ldots, \\mathbf w_k}{\\text{minimise}} &\\quad \\mathbb E \\| \\mathbf x - \\sum_{i=1}^k \\mathbf w_k \\mathbf w_k^\\top \\mathbf x \\|^2 \\\\\n",
    "\\text{subject to} &\\quad \\| \\mathbf w_i \\| = 1 &\\quad i = 1, \\ldots, k \\\\\n",
    "&\\quad \\mathbf w_i^\\top \\mathbf w_j = 0 &\\quad i \\neq j\n",
    "\\end{align*}\n",
    "\n",
    "- Equivalent to [simultaneous variance maximisation](#Simultaneous)\n",
    "- Optimal $\\mathbf w_i$ are the first $k$ eigenvectors $\\mathbf u_i$ of $\\boldsymbol\\Sigma$\n",
    "- $\\displaystyle \\mathbb E \\| \\mathbf x - \\sum_{i=1}^k \\mathbf w_k \\mathbf w_k^\\top \\mathbf x \\|^2 = \\sum_{i=k+1}^d \\lambda_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low rank matrix approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data matrix\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\mathbf M}{\\text{minimise}} &\\quad \\| \\mathbf X - \\mathbf M \\|_F^2 = \\sum_{ij} \\left( (\\mathbf X)_{ij} - (\\mathbf M)_{ij} \\right)^2 \\\\\n",
    "\\text{subject to} &\\quad \\mathrm{rank}(\\mathbf M) = k &\\quad k<r=\\mathrm{rank}(\\mathbf X)\n",
    "\\end{align*}\n",
    "\n",
    "- Optimal solution: $\\hat{\\mathbf X} = \\mathbf U_k \\mathbf S_k \\mathbf V_k^\\top$ (truncated singular value decomposition)\n",
    "- Singular values relate to eigenvalues: $\\displaystyle \\lambda_i = \\frac{s_i^2}{n}$\n",
    "- Right singular vectors $\\mathbf v_i$ are eigenvectors and hence PC directions\n",
    "- PC scores: $\\mathbf z_i = \\mathbf{Xv}_i = \\mathbf{USV}^\\top \\mathbf{Ve}_i = \\mathbf{USe}_i = s_i \\mathbf u_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample covariance matrix\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\mathbf M}{\\text{minimise}} &\\quad \\| \\boldsymbol\\Sigma - \\mathbf M \\|_F \\\\\n",
    "\\text{subject to} &\\quad \\mathrm{rank}(\\mathbf M) = k &\\quad k<r=\\mathrm{rank}(\\mathbf X) \\\\\n",
    "&\\quad \\mathbf M^\\top = \\mathbf M\n",
    "\\end{align*}\n",
    "\n",
    "- Optimal solution: $\\hat{\\boldsymbol\\Sigma} = \\mathbf V_k \\boldsymbol\\Lambda_k \\mathbf V_k^\\top$ (eigendecomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gram matrix\n",
    "\n",
    "1. Eigendecomposition: $\\displaystyle \\underset{n\\times n}{\\mathbf G} = \\mathbf X \\mathbf X^\\top \\overset{\\text{SVD}}{=} (\\mathbf{USV}^\\top) (\\mathbf{USV}^\\top)^\\top = \\mathbf{USS}^\\top \\mathbf U^\\top = \\mathbf U \\tilde{\\boldsymbol\\Lambda} \\mathbf U^\\top$\n",
    "1. Approximate $\\mathbf G$ like [sample cov](#Sample-covariance-matrix): $\\displaystyle \\hat{\\mathbf G} = \\sum_{i=1}^k s_i^2 \\mathbf u_i \\mathbf u_i^\\top = \\sum_{i=1}^k \\mathbf z_i \\mathbf z_i^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "\\begin{align*}\n",
    "    \\underset{K \\times 1}{\\mathbf z} &\\sim \\mathcal N(\\mathbf 0, \\mathbf I_K) \\\\\n",
    "    \\underset{D \\times 1}{\\boldsymbol\\varepsilon} &\\sim \\mathcal N(\\mathbf 0, \\sigma^2 \\mathbf I_D) \\\\\n",
    "    \\underset{D \\times 1}{\\mathbf x} &= \\underset{D \\times K}{\\mathbf W} \\ \\underset{K \\times 1}{\\mathbf z} + \\underset{D \\times 1}{\\boldsymbol\\mu} + \\underset{D \\times 1}{\\boldsymbol\\varepsilon}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf x | \\mathbf z) &= \\mathcal N(\\mathbf x; \\mathbf{Wz}+\\boldsymbol\\mu, \\sigma^2 \\mathbf I_D) \\\\\n",
    "p(\\mathbf x) &= \\mathcal N(\\mathbf x; \\boldsymbol\\mu, \\mathbf{WW}^\\top+\\sigma^2 \\mathbf I_D)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum likelihood\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf W_{\\text{ML}} &= \\mathbf U_k (\\boldsymbol\\Lambda_k - \\sigma^2 \\mathbf I)^{1/2} \\mathbf R \\\\\n",
    "\\sigma^2_{\\text{ML}} &= \\frac{1}{d-k} \\sum_{i=k+1}^d \\lambda_i\n",
    "\\end{align*}\n",
    "\n",
    "- $\\mathbf U_k$ are $k$ principal eigenvectors of $\\hat{\\boldsymbol\\Sigma} = \\mathrm{cov}(\\mathbf X) = \\frac{1}{n}\\mathbf{XX}^\\top$\n",
    "- $\\boldsymbol\\Lambda_k$ is diagonal matrix with eighenvalues\n",
    "- $\\mathbf R$ is arbitrary orthogonal matrix, interpreted as a rotation in the latent space, indicating not unique solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation to PCA\n",
    "\n",
    "$$p(\\mathbf z | \\mathbf x) = \\mathcal N(\\mathbf z; \\mathbf M^{-1} \\mathbf W^\\top (\\mathbf x - \\boldsymbol\\mu), \\sigma^2 \\mathbf M^{-1})$$\n",
    "\n",
    "- $\\mathbf M = \\mathbf W^\\top \\mathbf W + \\sigma^2 \\mathbf I$\n",
    "- Projection $\\displaystyle \\hat{\\mathbf x} = \\mathbf W_{\\text{ML}} \\mathbb E[\\mathbf z|\\mathbf x] = \\mathbf W_{\\text{ML}} \\mathbf M^{-1}_{\\text{ML}} \\mathbf W^\\top_{\\text{ML}} \\mathbf x \\to \\mathbf U_k \\mathbf U_k \\mathbf U_k^\\top \\mathbf x$ as $\\sigma^2 \\to 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear\n",
    "\n",
    "- Does not take manifold structure of data into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data points\n",
    "\n",
    "1. Observed data $\\tilde{\\mathbf X} = (\\tilde{\\mathbf x}_1, \\ldots, \\tilde{\\mathbf x}_n)$\n",
    "1. Centre data $\\mathbf X = \\tilde{\\mathbf X} \\mathbf C_n$ [(Standardisation)](#Standardisation)\n",
    "1. Compute PC scores $\\mathbf X = \\mathbf U_k^\\top \\mathbf X$, via computation of PC directions\n",
    "1. Compute PC scores $\\mathbf Z = \\sqrt{\\tilde{\\boldsymbol\\Lambda}_k} \\mathbf V_k^\\top$ directly from [Gram matrix](#Gram-matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner products\n",
    "\n",
    "$$\\mathbf G = \\mathbf X^\\top \\mathbf X = \\mathbf C_n^\\top \\tilde{\\mathbf X}^\\top \\tilde{\\mathbf X} \\mathbf C_n = C_n \\tilde{\\mathbf G} \\mathbf C_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distances\n",
    "\n",
    "$$\\mathbf G = -\\frac{1}{2} \\mathbf C_n \\boldsymbol\\Delta \\mathbf C_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "\n",
    "**New data matrix:**\n",
    "\n",
    "$$\\boldsymbol\\Phi = (\\boldsymbol\\phi_1, \\ldots, \\boldsymbol\\phi_n) = (\\phi(\\mathbf x_1), \\ldots, \\phi(\\mathbf x_n))$$\n",
    "\n",
    "**Uncentred Gram matrix:**\n",
    "\n",
    "$$(\\tilde{\\mathbf G})_{ij} = \\boldsymbol\\phi_i^\\top \\boldsymbol\\phi_j = \\phi(\\mathbf x_1)^\\top \\phi(\\mathbf x_j) = k(\\mathbf x_i, \\mathbf x_j)$$\n",
    "\n",
    "**Example kernels:**\n",
    "- Polynomial kernel: $k(\\mathbf x, \\mathbf x') = (\\mathbf x^\\top \\mathbf x')^a$\n",
    "- Gaussian kernel: $\\displaystyle k(\\mathbf x, \\mathbf x') = \\exp \\left( -\\frac{\\| \\mathbf x - \\mathbf x' \\|^2}{2\\sigma^2} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric MDS\n",
    "\n",
    "$$\\underset{\\mathbf z_1, \\ldots, \\mathbf z_n}{\\text{minimise}} \\quad \\sum_{i<j} w_{ij} ( \\| \\mathbf z_i - \\mathbf z_j \\| - \\delta_{ij})^2$$\n",
    "\n",
    "- $\\delta_{ij}$ are dissimilarities measuring difference between two data items, e.g. Euclidean distances. \n",
    "- $w_{ij} \\geq 0$ specified by user\n",
    "- If $\\displaystyle w_{ij} = \\frac{1}{\\delta_{ij}}$, Sammon nonlinear mapping, emphasising the faithful representation of small dissimilarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonmetric MDS\n",
    "\n",
    "$$\\underset{\\mathbf z_1, \\ldots, \\mathbf z_n, f}{\\text{minimise}} \\quad \\sum_{i<j} w_{ij} \\left( \\| \\mathbf z_i - \\mathbf z_j \\| - f(\\delta_{ij}) \\right)^2$$\n",
    "\n",
    "- Only relation between $\\delta_{ij}$ matter\n",
    "- $f$ is monotonic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical MDS\n",
    "\n",
    "- $\\boldsymbol\\Delta$ is distance matrix between the unknown vectors, not necessarily postive semidefinite\n",
    "- Follow [this steps](#Distances)\n",
    "- Choose small $k$ to avoid negative eigenvalues\n",
    "\n",
    "- Alternatively, approximate negative definite $\\boldsymbol\\Delta$ by\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\mathbf M}{\\text{minimise}} &\\quad \\left\\| (-\\frac{1}{2} \\mathbf C_n \\boldsymbol\\Delta \\mathbf C_n) - \\mathbf M^\\top \\mathbf M \\right\\|_F \\\\\n",
    "\\text{subject to} &\\quad \\mathrm{rank}(\\mathbf M^\\top \\mathbf M) = k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap\n",
    "\n",
    "- Use geodesic distances representing the shortest paths along the curved surface\n",
    "\n",
    "\n",
    "1. Find the neighbours of each data point in high-dimensional data\n",
    "1. Compute the geodesic pairwise distances between all points\n",
    "1. Embed the data via MDS so as to preserve these distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modelling and Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction loss\n",
    "\n",
    "$$\\mathcal J(h) = \\mathbb E_{p(\\hat y, y)} [\\mathcal L(\\hat y, y)] = \\mathbb E_{p(\\mathbf x, y)} [\\mathcal L(h(\\mathbf x), y)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loss\n",
    "\n",
    "$$J_{\\boldsymbol\\lambda}^* = \\min_{\\boldsymbol\\theta} J_{\\boldsymbol\\lambda}(\\boldsymbol\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(h_{\\boldsymbol\\lambda}(\\mathbf x_i; \\boldsymbol\\theta), y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalisation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalisation loss\n",
    "\n",
    "**For prediction functions:**\n",
    "\n",
    "$$\\mathcal J(\\hat h) = \\mathbb E_{p(\\mathbf x, y)} \\left[ \\mathcal L(\\hat h(x), y) \\right]$$\n",
    "\n",
    "**For algorithms:**\n",
    "\n",
    "$$\\bar{\\mathcal J}(\\mathcal A) = \\mathbb E_{p(\\mathcal D^{\\text{train}})} \\left[ \\mathcal J (\\hat h) \\right] = \\mathbb E_{p(\\mathcal D^{\\text{train}})} \\left[ \\mathcal J(\\mathcal A(\\mathcal D^{\\text{train}})) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting and underfitting\n",
    "\n",
    "Overfitting: reduce complexity of model $\\to$ reduce prediction loss\n",
    "\n",
    "Underfitting: choose more flexible model $\\to$ reduce prediction loss\n",
    "\n",
    "$$\\underset{\\boldsymbol\\theta}{\\text{minimise}} \\quad \\mathcal J_{\\boldsymbol\\lambda}(\\boldsymbol\\theta) + \\lambda_{\\text{reg}} R(\\boldsymbol\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter selection\n",
    "\n",
    "Consider both complexity and training size of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the generalisation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hold-out approach\n",
    "\n",
    "Prediction function:\n",
    "\n",
    "$$\\hat h = \\mathcal A(\\mathcal D^{\\text{train}})$$\n",
    "\n",
    "Prediction loss on $\\tilde{\\mathcal D}$ (validation/test set):\n",
    "\n",
    "$$\\hat{\\mathcal J}(\\hat h; \\tilde{\\mathcal D}) = \\frac{1}{\\tilde n} \\sum_{i=1}^{\\tilde n} \\mathcal L(\\hat h(\\tilde{\\mathbf x_i}), \\tilde y_i)$$\n",
    "\n",
    "- Common split ratios: 6:4, 7:3 or 8:2\n",
    "- Increase ratio if the number of hyperparameter is large\n",
    "- Split randomly\n",
    "- Stratification (classes are in the same proportions in both sets)\n",
    "- Drawback (use cross-validation to avoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-validation\n",
    "\n",
    "$K$-fold:\n",
    "\n",
    "$$\\mathcal D_k^{\\text{train}} = \\bigcup_{i \\neq k} \\mathcal D_i \\quad \\mathcal D_k^{\\text{val}} = \\mathcal D_k$$\n",
    "\n",
    "Prediction function and loss for the $k$-th fold:\n",
    "\n",
    "$$\\hat h_k = \\mathcal A(\\mathcal D_k^{\\text{train}}) \\quad \\hat{\\mathcal J_k} = \\hat{\\mathcal J}(\\hat h_k; \\mathcal D_k^{\\text{val}})$$\n",
    "\n",
    "Cross-validation score:\n",
    "\n",
    "$$\\mathrm{CV} = \\frac{1}{K} \\sum_{i=1}^K \\hat{\\mathcal J_k} = \\frac{1}{K} \\sum_{i=1}^K \\hat{\\mathcal J} \\left( \\mathcal A(\\mathcal D_k^{\\text{train}}); \\mathcal D_k^{\\text{val}} \\right) = \\hat{\\bar{\\mathcal J}}(\\mathcal A)$$\n",
    "\n",
    "Estimated variance of CV score:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\displaystyle \\mathrm{Var}(\\mathrm{CV}) \\approx \\frac{1}{K} \\mathrm{Var}(\\hat{\\mathcal J}) \\\\\n",
    "\\displaystyle \\mathrm{Var}(\\hat{\\mathcal J}) \\approx \\frac{1}{K} \\sum_{k=1}^K (\\hat{\\mathcal J_k} - \\mathrm{CV})^2\n",
    "\\end{cases}$$\n",
    "\n",
    "indeed, $\\displaystyle \\hat{\\mathrm{Var}}(\\mathrm{CV}) = \\frac{1}{K(K-1)} \\sum_{k=1}^K (\\hat{\\mathcal J_k} - \\mathrm{CV})^2$\n",
    "\n",
    "- LOOCV: generally expensive, but possible in e.g. Gaussian time series and spatial models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter selection and performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two times hold-out\n",
    "\n",
    "1. Split off test set $\\mathcal D^{\\text{test}}$ for estimating the performance of final prediction function\n",
    "1. Split remaining data into $\\mathcal D^{\\text{train}}$ and $\\mathcal D^{\\text{val}}$\n",
    "1. Tune parameters $\\boldsymbol\\lambda$ in an algorithm $\\displaystyle \\hat h_{\\boldsymbol\\lambda} = \\mathcal A_{\\boldsymbol\\lambda}(\\mathcal D^{\\text{train}})$\n",
    "1. Compute prediction loss $\\displaystyle \\mathrm{PL}(\\boldsymbol\\lambda) = \\hat{\\mathcal J}(\\hat h_{\\boldsymbol\\lambda}; \\mathcal D^{\\text{val}})$ and choose $\\displaystyle \\hat{\\boldsymbol\\lambda} = \\underset{\\boldsymbol\\lambda}{\\arg\\min} \\mathrm{PL}(\\boldsymbol\\lambda)$\n",
    "1. Re-estimate on union of training and validation data $\\displaystyle \\hat h = \\mathcal A_{\\hat{\\boldsymbol\\lambda}} (\\mathcal D^{\\text{train}} \\cup \\mathcal D^{\\text{val}})$\n",
    "1. Compute prediction loss on test data $\\displaystyle \\hat{\\mathcal J} = \\hat{\\mathcal J}(\\hat h; \\mathcal D^{\\text{test}})$\n",
    "1. Re-estimate on all data $\\displaystyle \\hat h_{\\boldsymbol\\lambda} = \\mathcal A_{\\hat{\\boldsymbol\\lambda}} (\\mathcal D)$\n",
    "\n",
    "\n",
    "- Re-estimation sometimes needs to be skipped\n",
    "- Optimisation methods over $\\boldsymbol\\lambda$: grid search, random search, Bayesian optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-validation and hold-out\n",
    "\n",
    "1. Split off test set $\\mathcal D^{\\text{test}}$ for estimating the performance of final prediction function\n",
    "1. Compute CV score on remaining data $\\mathcal D^{\\text{train}}$: $\\displaystyle \\mathrm{EPL}(\\boldsymbol\\lambda) = \\mathrm{CV} = \\hat{\\bar{\\mathcal J}} (\\mathcal A_{\\boldsymbol\\lambda})$\n",
    "1. Choose $\\displaystyle \\hat{\\boldsymbol\\lambda} = \\underset{\\boldsymbol\\lambda}{\\arg\\min} \\mathrm{EPL}(\\boldsymbol\\lambda)$ OR simplest model and $\\displaystyle \\mathrm{EPL}(\\hat{\\boldsymbol\\lambda}) \\in \\left[\\min \\mathrm{CV} \\pm \\sqrt{\\mathrm{Var}(\\min \\mathrm{CV})}\\right]$\n",
    "1. Re-estimate on whole training data $\\displaystyle \\hat h = \\mathcal A_{\\hat{\\boldsymbol\\lambda}}(\\mathcal D^{\\text{train}})$\n",
    "1. Compute prediction loss on test data $\\displaystyle \\hat{\\mathcal J} = \\hat{\\mathcal J}(\\hat h; \\mathcal D^{\\text{test}})$\n",
    "1. Re-estimate on all data $\\displaystyle \\hat h_{\\boldsymbol\\lambda} = \\mathcal A_{\\hat{\\boldsymbol\\lambda}} (\\mathcal D)$\n",
    "\n",
    "\n",
    "- Minimal CV score is an optimistic estimate of prediction loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In regression\n",
    "\n",
    "- Square loss: $\\displaystyle L(\\hat y, y) = \\frac{1}{2} (\\hat y - y)^2$\n",
    "- Absolute loss: $\\displaystyle L(\\hat y, y) = |\\hat y - y|$ (more robust than square loss because not grow quickly, but not differentiable)\n",
    "- Huber loss: $\\displaystyle L(\\hat y, y) = \\begin{cases}\n",
    "\\displaystyle \\frac{1}{2} (\\hat y - y)^2 & \\text{if } |\\hat y - y| < \\delta \\\\\n",
    "\\displaystyle \\delta |y - \\hat y| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "\\end{cases}$ (combimes good properties of square and absolute loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-differentiable\n",
    "\n",
    "$$\\mathbf L_{\\hat y, y} = \\begin{pmatrix}\n",
    "L(1,1) & L(1,2) & \\cdots & L(1,K) \\\\\n",
    "L(2,1) & L(2,2) & \\cdots & L(2,K) \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "L(K,1) & L(K,2) & \\cdots & L(K,K)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "**Zero-one loss:**\n",
    "\n",
    "$$L(i,j) = \\begin{cases}\n",
    "1 & i \\neq j \\\\\n",
    "0 & i = j\n",
    "\\end{cases}$$\n",
    "\n",
    "- Expectation: $\\displaystyle \\mathcal J(h) = \\sum_{i \\neq j} p(i,j) = \\mathbb P(i\\neq j)$ (misclassification/error rate)\n",
    "\n",
    "**Binary classification:**\n",
    "\n",
    "| $\\hat y$ | $y$ | event | joint probability $p(\\hat y, y)$ | shorthand notation | rate | conditional probability $p(\\hat y|y)$ |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 1 | 1 | True Positive | $\\mathbb P(\\hat y=1, y=1)$ | $p(1,1)$ | TP rate<br>sensitivity, hit rate, recall | $\\mathbb P(\\hat y=1 | y=1)$ |\n",
    "| 1 | -1 | False Positive | $\\mathbb P(\\hat y=1, y=-1)$ | $p(1,-1)$ | FP rate<br>type 1 error, fall-out | $\\mathbb P(\\hat y=1 | y=-1)$ |\n",
    "| -1 | 1 | False Negative | $\\mathbb P(\\hat y=-1, y=1)$ | $p(-1,1)$ | FN rate<br>type 2 error | $\\mathbb P(\\hat y=-1 | y=1)$ |\n",
    "| -1 | -1 | True Negative | $\\mathbb P(\\hat y=-1, y=-1)$ | $p(-1, -1)$ | TN rate<br>specificity | $\\mathbb P(\\hat y=-1 | y=-1)$ |\n",
    "\n",
    "**Loss function penalising FP and FN rates:**\n",
    "\n",
    "$$\\mathbf L_{\\hat y, y} = \\begin{pmatrix}\n",
    "0 & \\frac{1}{\\mathbb P(y=1)} \\\\\n",
    "\\frac{1}{\\mathbb P(y=-1)} & 0 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "- Expectation: $\\displaystyle \\mathcal J(h) = \\sum_{i,j} L(i,j) p(i,j) = \\mathrm{FPR} + \\mathrm{FNR}$\n",
    "- Not meaningful because optimal solution is $\\hat y=-1$: $\\mathrm{TPR}=0$\n",
    "\n",
    "**Reciver operating characteristic (ROC):**\n",
    "\n",
    "- TPR vs FPR\n",
    "- Perfect classifier at upper-left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Differentiable\n",
    "\n",
    "Assume $\\hat y(\\mathbf x) = \\mathrm{sign}(h(\\mathbf x))$, correct classification equivalent to margin $yh(\\mathbf x) >0$\n",
    "\n",
    "- Zero-one loss: $\\displaystyle L(h(\\mathbf x), y) = \\begin{cases}\n",
    "1 & \\text{if } yh(\\mathbf x) < 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "- Square loss: $L(h(\\mathbf x), y) = (1-yh(\\mathbf x))^2$ (sensitive to outliers and penalises large margins)\n",
    "- Log loss: $L(h(\\mathbf x), y) = \\log (1+\\exp (-yh(\\mathbf x)))$ (equivalent to maximising log-likelihood in logistic regression)\n",
    "- Exponential loss: $L(h(\\mathbf x), y) = \\exp(-yh(\\mathbf x))$\n",
    "- Hinge loss: $L(h(\\mathbf x),y) = \\max(0, 1-yh(\\mathbf x))$\n",
    "- Squared hinge loss: $L(h(\\mathbf x),y) = \\max(0, 1-yh(\\mathbf x))^2$ (differentiable everywhere)\n",
    "- Huberised squared hinge loss: $\\displaystyle L(h(\\mathbf x),y) = \\begin{cases}\n",
    "-4yh(\\mathbf x) & \\text{if } yh(\\mathbf x) < -1 \\\\\n",
    "\\max(0, 1-yh(\\mathbf x))^2 & \\text{otherwise}\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** s1680642\n",
    "\n",
    "**Licensing:** <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
